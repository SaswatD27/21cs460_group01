<html>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>

    body {
      margin:0;
      padding:0;
      font-family: "Helvetica Neue", "Segoe UI", -apple-system, BlinkMacSystemFont, Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
      background-color:white;
    }

    .plot {
      width: 100%;
      height: 100%;
    }

    .top-container {
      background-color: #f1f1f1;
      padding: 30px;
      margin: 0%;
    }

    .header {
      padding: 10px 16px;
      background: rgb(0, 0, 0);
      text-align: center;
      color: #f1f1f1;
      border:0%
    }

    .content {
      padding: 16px;
      margin: 15%;
      margin-top: 0%;
    }

    .sticky {
      position: fixed;
      top: 0;
      width: 100%;
    }

    .sticky + .content {
      padding-top: 102px;
      margin: 15%;
      margin-top: 0%;
    }

    /* Three image containers (use 25% for four, and 50% for two, etc) */
    .column {
    float: left;
    width: 33.33%;
    padding: 5px;
    margin-left: 8%;
    }

    /* Clear floats after image containers */
    .row::after {
    content: "";
    clear: both;
    display: table;
    }

    img{
    display: block;
    margin-left: auto;
    margin-right: auto;
    }

    #timeline {
      font-family: Arial, Helvetica, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }

    #timeline td, #timeline th {
      border: 1px solid #ddd;
      padding: 8px;
    }

    #timeline tr:nth-child(even){background-color: #f2f2f2;}

    #timeline tr:hover {background-color: #ddd;}

    #timeline th {
      padding-top: 12px;
      padding-bottom: 12px;
      text-align: left;
      background-color:black;
      color: white;
    }
    </style>
</head>
<div>
<div class="container"><header>
<h1 style="text-align: center;">Course Project, CS460, Fall 2021-22</h1>
<h2 style="text-align: center;">On Federated Learning and Possible Improvements</h2>
</header>
<p style="text-align: center;">Saswat Das <br />1811138 | saswat.das@niser.ac.in | School of Mathematical Sciences, NISER, HBNI <br />Suraj Ku. Patel<br />1811163 | suraj.kpatel@niser.ac.in | School of Physical Sciences, NISER, HBNI <br /><br /><a href="https://github.com/SaswatD27/CS460-ML-Project-2021">Project Github Repository</a></p>
<hr />
<h3>1. Project Proposal (6 Sept 2021)</h3>
<h4>Introduction</h4>
<img style="display: block; margin-left: auto; margin-right: auto;" src="https://blog.ml.cmu.edu/wp-content/uploads/2019/11/Screen-Shot-2019-11-12-at-10.41.38-AM-970x377.png" alt="Federated Learning Schematic" width="500" />
<p>&nbsp;</p>
<p><em>Federated learning</em> (abbreviated as <em>FL</em>) refers to the practice of conducting machine learning based on several users' data by having each user, given an initial global model by a central server, <em>locally train&nbsp;</em>a model on their local devices, then communicate their locally trained model to the central server, following which each user's local model is&nbsp;<em>aggregated&nbsp;</em>at the central server with that of others, and used to update the global model.</p>
<p>This is in contrast to centralised learning, where the users' data would have been sent to the central server, and a model would have been trained on all that collected data in one central server.</p>
<h5>Advantages?</h5>
<p>Quite some!</p>
<ol>
<li>The user's personal data never leaves their local device, thus ensuring a healthy measure of privacy;</li>
<li>Can possibly incorporate nuances of each user's local model by a suitable aggregation technique/algorithm to provide better, more nuanced learning.</li>
</ol>
<h5>Concerns/Areas of Possible Improvement</h5>
<ul>
<li>Communication costs post local training and optimising number of communication rounds between each local device and the central server:</li>
<li>Cost of computation due to local training on each device and effects on user experience during local training;</li>
<li>Possible improvements in aggregation algorithms;</li>
<li>Enhancement of privacy, and making sure no adversary can conclude from the (say, weights of the) local models an individual user's personal information or behaviour.</li>
</ul>
<h4>What we intend to do - a brief sketch</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Implement a Federated Learning algorithm (tentatively </span><span style="font-weight: 400;">FederatedAveraging</span><span style="font-weight: 400;">) after locally training models using algorithms like SGD (as suggested) on generated datasets, or even implementing some neural network(s) if possible using popular datasets (viz. FEMNIST, Shakespeare, Sentiment140, CIFAR 10) if we are feeling bold. We can use TensorFlow and SherpaAI for this if possible/needed (no promises).</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Using the (local) learning algorithm as a baseline, we can compare the results of our FL based trained-and-aggregated model with centralised learning with the same algorithm, and some other popular algorithms.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;"><span style="font-weight: 400;">Suggest improvements to the paradigm just implemented in terms of</span></span>
<ol>
<li>The aggregating formula/algorithm;</li>
<li>minimising rounds of communication and computational cost per local device; and/or</li>
<li>enhancing privacy (viz. by adding differential privacy into the mix).</li>
</ol>
</li>
</ul>
<p><span style="font-weight: 400;">Then we will see how said tweaks work in terms of these metrics and report any observed improvements/changes (fingers crossed).</span></p>
<h4>Midway Targets</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Successfully implement an FL model, and come up with some preliminary results;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Suggest some tweaks and improvements to test out;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">If needed, present pertinent concepts from relevant literature.</span></li>
</ul>
<h4>Work Division</h4>
<ol>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Coding/Privacy based considerations - Joint task, will divide up coding of different components fluidly as needed.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Data analysis and visualisation - Suraj</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Mathematics, formulae, proofs - Saswat</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Report writing and project website maintenance - Joint task</span></li>
</ol>
<h4>Expected Results</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Performance of FL based learning: almost as sound as if the learning was centralised, and maybe more nuanced.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Either of these: Increased privacy, less communication rounds with the server, less/quicker local device computation, better aggregation.</span></li>
</ul>
<h4>Relevant Papers</h4>
<ul>
<li aria-level="1">McMahan, H. B. et al. &ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data.&rdquo; AISTATS (2017).</li>
<li aria-level="1">Konecn&yacute;, Jakub et al. &ldquo;Federated Learning: Strategies for Improving Communication Efficiency.&rdquo; ArXiv abs/1610.05492 (2016): n. pag.</li>
<li>Barroso, Nuria Rodr&iacute;guez et al. &ldquo;Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy.&rdquo; Inf. Fusion 64 (2020): 270-292.</li>
</ul>
<hr />
<h4>Project Presentation Slides</h4>
<embed style="background-image: url('img/object.gif'); display: block; margin-left: auto; margin-right: auto;" src="https://drive.google.com/viewerng/viewer?embedded=true&amp;url=https://github.com/SaswatD27/CS460-ML-Project-2021/raw/main/CS460%20-%20Project%20Proposal.pdf" width="500" height="375"></embed><hr /></div>
</div>
<h3>2. Project Midway</h3>
<h4>Literature Review</h4>
<p>Some of the most important papers we referred to are listed below.</p>
<ul>
<li>McMahan, H. Brendan et al. &ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data.&rdquo;&nbsp;<em>AISTATS</em>&nbsp;(2017).</li>
<li>Konecn&yacute;, Jakub et al. &ldquo;Federated Learning: Strategies for Improving Communication Efficiency.&rdquo;&nbsp;<em>ArXiv</em> abs/1610.05492 (2016).</li>
<li>Wei, Kang et al. &ldquo;Federated Learning With Differential Privacy: Algorithms and Performance Analysis.&rdquo;&nbsp;<em>IEEE Transactions on Information Forensics and Security</em>&nbsp;15 (2020): 3454-3469.</li>
<li>Ji, Shaoxiong et al. &ldquo;Dynamic Sampling and Selective Masking for Communication-Efficient Federated Learning.&rdquo;&nbsp;<em>ArXiv</em> abs/2003.09603 (2020).</li>
</ul>
<ol>
<li><strong>McMahan et al</strong>
<ul>
<li>
<p>Introduces federated learning;</p>
</li>
<li>
<p>Addresses issues such as unbalanced volume of datapoints across all clients, limited communication capabilities of clients (via multiple rounds of communication), and the mass distributed nature of the clients, and the non-IID nature of data as an individual&rsquo;s data is specific;</p>
</li>
<li>
<p>Introduces <code>FedAvg</code> (<span class="math inline">\(w_{t+1}\gets\sum_{i=1}^K\frac{n_k}{n}w_{t+1}^k\)</span>);</p>
</li>
<li>
<p>Talks about controlling parallelism of local computation and increased local computation by varying batch size for gradient descent (decreasing it increases amount/precision of computation, no. of rounds, and no. of clients queried per round (affects parallelism).</p>
</li>
</ul>
</li>
<li><strong>Wei et al</strong>
<ul>
<li>
<p>Introduces Gaussian noise w.r.t. a clipping parameter <span class="math inline">\(C\)</span> to weight vector uploads, weights scaled w.r.t. <span class="math inline">\(C\)</span>;</p>
</li>
<li>
<p>Motivation: Na&iuml;vely uploaded weights carry the risk of being used by adversaries to compromise users;</p>
</li>
<li>
<p>Proposes uplink and optional downlink noise addition;</p>
</li>
<li>
<p>Takes fairly large values for the privacy budget;</p>
</li>
<li>
<p>Accuracy improves with no. of clients queried and rounds of communication;</p>
</li>
<li>
<p>Best accuracy when clients have (near) identical amounts of quality data;</p>
</li>
<li>
<p>Akin to central differential privacy, straightforward noise addition.</p>
</li>
</ul>
</li>
<li><strong>Konečn&yacute; et al</strong>
<ul>
<li>
<p>Introduces methods to reduce the uplink communication costs by reducing the size of the updated model sent back by the client to the server.</p>
</li>
<li>
<p>Motivation: Poor bandwidth/expensive connections of a number of the participating devices (clients) leads to problems during the aggregation of data for FL.</p>
</li>
<li>
<p>Two methods for sending a smaller model are:</p>
<ul>
<li>
<p><em>Structured updates</em>, updates are from a restricted space and can be parametrized using a smaller number of variables. Algorithms: Low Rank and Random Mask.</p>
</li>
<li>
<p><em>Sketched updates</em>, full model updates are learned but a compressed model update is sent to the server. Algorithms: Subsampling, Probabilistic quantization, and structured random rotations.</p>
</li>
</ul>
</li>
<li>
<p><code>FedAvg</code> is used for the experiments to decrease the number of rounds of communication required to train a good model.</p>
</li>
<li>
<p>Conclusion of the paper</p>
<ul>
<li>
<p>Random mask performs significantly better than low rank, as the size of the updates is reduced.</p>
</li>
<li>
<p>Random masking gives higher accuracy as compared to sketched updates method but reaches moderate accuracy much slower.</p>
</li>
<li>
<p>Quantization algorithm alone is very unstable For a small number of quantization bits and smaller modes, and random rotation with quantization is more stable and has improved performance as compared to without rotation.</p>
</li>
<li>
<p>By increasing the number of rounds of training, the fraction of clients taken per round can be minimised without risking accuracy.</p>
</li>
<li>
<p>Introduced an important and practical tradeoff in the FL: one can select more clients in each round while having each of them communicate less, and obtain the same accuracy as using fewer clients, but having each of them communicate more.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>Extra:<strong> Ji et al</strong>
<ul>
<li>Proposes dynamic sampling and selective mask;</li>
<li>Dynamic sampling: In contrast to the usual way of sampling clients during each round of communication (i.e. static sampling), we define a decay constant, called <span class="math inline">\(\beta\), and if the initial sampling rate is <span class="math inline">\(C\)</span>, then we choose <span class="math inline">\(\frac{C}{\exp(\beta t)}\)</span> fraction of clients for round <span class="math inline">\(t\)</span> (starting with <span class="math inline">\(t=0\)</span>)</span>;</li>
<li>Selective mask: Instead of fully (uniformly) randomised selection of indices for a mask, choose the top <span class="math inline">\(k\)</span> indices with the most significant updates to the initial model for each client;</li>
<li>Selective mask consistently outperformed random mask in terms of accuracy and rate of convergence;</li>
<li>Dynamic sampling gave accuracy comparable to static sampling, but at a significant loss in communication cost.</li>
</ul>
</li>
</ol>
<h4>Techniques Used/Explored</h4>
<ul>
<li>
<p>For aggregation, we used <code>FedAvg</code> from for the most vanilla FL implementation, and then added to it as necessary for the implementation of more sophisticated FL techniques;</p>
</li>
<li>
<p>The Gaussian Mechanism to provide <span class="math inline">\((\varepsilon,\delta)\)</span>-differential privacy pre-upload from each device (Noising before Aggregation), as introduced by Wei et al.;</p>
</li>
<li>
<p><em>Static Sampling</em> of clients for every round of training to reduce communication rounds per user;</p>
</li>
<li>
<p><em>Random Mask</em> and <em>Probabilistic Quantisation</em> as described by Konečn&yacute; et al;</p>
</li>
<li>
<p><em>Dynamic Sampling</em> of clients to successively reduce the proportion of clients involved in successive rounds of communication to further save on rounds of communication and computational resources (as proposed by Ji et al);</p>
</li>
<li>
<p>Considering/considered using: <em>Selective Mask</em> from (especially when working with larger weight vectors/matrices).</p>
</li>
</ul>
<h4>Experiments and Observations</h4>
<p>We implemented <code>FedAvg</code> from scratch in Python, with a 0.25 static sampling rate, with anywhere between 3 to 5 (or more) rounds of calling for training from sampled (w.r.t. the uniform distribution) clients, with local training involving multiple linear regression implemented via Stochastic Gradient Descent on a synthetically generated dataset for about 500 clients, with static subsampling (contrast with dynamic subsampling).</p>
<p>The generated datasets, generated around a pre-chosen/generated "true weights vector" (we chose the dimension of these weight vectors to be 7) (e.g. <span class="math inline">\([1, 2, 4, 3, 5, 6, 1.2]\)</span>), are mostly IID as we focused more on implementing a model atop that.</p>
<p>As we are using regression here, we use, for a particular instance, average training/testing error per data point as a metric of a deployment&rsquo;s accuracy, and simply measure the time taken by running it on Google Colab and compare them as a rough measure of how quick each is, and implement centralised learning to serve as a baseline for our exploration of these FL paradigms.</p>
<p>We then tried the above listed techniques, sometimes standalone or in conjunction with each other as follows.</p>
<ol>
<li><strong>Centralised Learning (As a Baseline)</strong><br />
<p>For centralised learning, we simply gathered and flattened the list of all local datasets into a cumulative list of all datapoints, and ran SGD on it for 100 epochs.<br />Time Taken <span class="math inline">\(\approx 268-270\)</span> seconds.<br />Average Training Error for Centralised Learning on SGD <span class="math inline">\(\approx 3.2472\times 10^{-29}\).</span><br />Average Testing Error for Centralised Learning on SGD <span class="math inline">\(\approx 3.2154 \times 10^{-29}\).</span></p>
</li>
<li><strong>Vanilla <code>FederatedAveraging/FedAvg</code></strong>
<p>We then ran vanilla <code>FedAvg</code> with static sampling of clients at a rate of 0.25 of the client population per round of training on the above mentioned local datasets, with 100 epochs per round of local training. Number of rounds, <span class="math inline">\(T=5\)</span>.<br />Time taken <span class="math inline">\(\approx 362\)</span> seconds.<br />Average Training Error for Vanilla FedAvg on SGD <span class="math inline">\(\approx 2.5331\times 10^{-18}\)</span>.<br />Average Testing Error for Vanilla FedAvg on SGD <span class="math inline">\(\approx 2.6021\times 10^{-18}\)</span>.</p>
</li>
<li><strong>Noising before Aggregation (From Wei et al)</strong>
<p>Adds Gaussian noise to appropriately clipped weights from each user. Motivation: To stop attacks involving reconstruction of raw data/private information from na&iuml;ve uploading of weight vectors/matrices. Optionally adds downlink DP, but we felt it was unnecessary.</p>
<p>We an <code>FedAvg</code> with static sampling of clients at a rate of 0.25 per round of training, clipping the locally generated weight vectors to gain bounds (defining upper bound of the norm of a weight vector as <span class="math inline">\(C =1.01\times\max(w_i)\)</span>, where <span class="math inline">\(w -\)</span> weight vector, for the sake of computing the sensitivity of queries for the weight vectors, then adding Gaussian noise calibrated to <span class="math inline">\(C,\varepsilon,\delta\)</span> to each weight (<span class="math inline">\(\varepsilon=70,\delta=0.1\)</span>) with 100 epochs per round of local training. Number of rounds, <span class="math inline">\(T=5\)</span>.<br />Time taken <span class="math inline">\(\approx 349\)</span> seconds (which makes it about as fast as vanilla <code>FedAvg</code>).<br />Average Training Error for NbA FedAvg on SGD <span class="math inline">\(\approx 7.2562\times 10^{-7}\)</span>.<br />Average Testing Error for NbA FedAvg on SGD <span class="math inline">\(\approx 6.6740\times10^{-7}\)</span>.</p>
</li>
<li><strong>NbAFL with Random Mask</strong>
<p>We use the same setup as for the above NbAFL implementation, but with a layer of uniformly chosen random masks, excluding <span class="math inline">\(\approx s=0.25\)</span> of the weights, prior to uploading by a client. Seems to consistently outperform base NbAFL in terms of accuracy, also reduces communication overhead!<br />Time taken <span class="math inline">\(\approx 356\)</span> seconds.<br />Average Training Error <span class="math inline">\(\approx 0.006601\).</span><br />Average Testing Error <span class="math inline">\(\approx 0.007577\).</span></p>
</li>
<li><strong>NbAFL with Probabilistic Binarisation</strong>
<p>We again use the same setup and parameters as our vanilla NbAFL implementation, but with probabilistic binarisation implemented atop it.<br />Time taken <span class="math inline">\(\approx 356\)</span> seconds.<br />Average Training Error <span class="math inline">\(\approx 0.021241\)</span>.<br />Average Testing Error <span class="math inline">\(\approx 0.019635\)</span></p>
</li>
<li><strong>NbAFL with Dynamic Sampling</strong>
<p>We again adopt the same setup as that for our vanilla NbAFL implementation, but instead of static sampling of clients, we sample clients with an initial rate of <span class="math inline">\(0.25\)</span>, which decays by a factor of <span class="math inline">\(\frac 1{\exp(\beta t)}\)</span>, where <span class="math inline">\(t =\)</span> number of rounds elapsed. We take <span class="math inline">\(\beta=0.05\)</span>.<br />
Time taken <span class="math inline">\(\approx 334\)</span> seconds.<br />
(Saves on time and no. of communication rounds!)<br />
Average Training Error <span class="math inline">\(\approx 0.010684\)</span>.<br />
Average Testing Error <span class="math inline">\(\approx  0.010908\)</span>.</p>
</li>
</ol>
