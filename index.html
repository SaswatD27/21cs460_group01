<html>

<head>
    <title>Course Project | Suraj Patel | Saswat Das | CS460, Machine Learning, Fall Semester, 2021-22</title>
    <style>
body {
  font-family: Verdana, sans-serif;
}
</style>
</head>
<div class="container"><header>
<h1>Course Project, CS460, Fall 2021-22</h1>
<h2>On Federated Learning and Possible Improvements</h2>
</header>
<p>Saswat Das <br />1811138 | saswat.das@niser.ac.in | School of Mathematical Sciences, NISER, HBNI <br />Suraj Ku. Patel<br />1811163 | suraj.kpatel@niser.ac.in | School of Physical Sciences, NISER, HBNI <br /><br /><a href="https://github.com/SaswatD27/CS460-ML-Project-2021">Project Github Repository</a></p>
<hr />
<h3>1. Project Proposal (6 Sept 2021)</h3>
<h4>Introduction</h4>
<img style="display: block; margin-left: auto; margin-right: auto;" src="https://blog.ml.cmu.edu/wp-content/uploads/2019/11/Screen-Shot-2019-11-12-at-10.41.38-AM-970x377.png" alt="Federated Learning Schematic" width="500" />
<p>&nbsp;</p>
<p><em>Federated learning</em> (abbreviated as <em>FL</em>) refers to the practice of conducting machine learning based on several users' data by having each user, given an initial global model by a central server, <em>locally train&nbsp;</em>a model on their local devices, then communicate their locally trained model to the central server, following which each user's local model is&nbsp;<em>aggregated&nbsp;</em>at the central server with that of others, and used to update the global model.</p>
<p>This is in contrast to centralised learning, where the users' data would have been sent to the central server, and a model would have been trained on all that collected data in one central server.</p>
<h5>Advantages?</h5>
<p>Many!</p>
<ol>
<li>The user's personal data never leaves their local device, thus ensuring a healthy measure of privacy;</li>
<li>Can possibly incorporate nuances of each user's local model by a suitable aggregation technique/algorithm to provide better, more nuanced learning.</li>
</ol>
<h5>Concerns/Areas of Possible Improvement</h5>
<ul>
<li>Communication costs post local training and optimising number of communication rounds between each local device and the central server:</li>
<li>Cost of computation due to local training on each device and effects on user experience during local training;</li>
<li>Possible improvements in aggregation algorithms;</li>
<li>Enhancement of privacy, and making sure no adversary can conclude the (say, weights of the) local models about an individual user's personal information or behaviour.</li>
</ul>
<h4>What we intend to do - a brief sketch</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Implement a Federated Learning algorithm (tentatively </span><span style="font-weight: 400;">FederatedAveraging</span><span style="font-weight: 400;">) after locally training models using algorithms like SGD (as suggested) on generated datasets, or even implementing some neural network(s) if possible using popular datasets (viz. FEMNIST, Shakespeare, Sentiment140, CIFAR 10) if we are feeling bold. We can use TensorFlow and SherpaAI for this if possible/needed (no promises).</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Using the (local) learning algorithm as a baseline, we can compare the results of our FL based trained-and-aggregated model with centralised learning with the same algorithm, and some other popular algorithms.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;"><span style="font-weight: 400;">Suggest improvements to the paradigm just implemented in terms of</span></span>
<ol>
<li>The aggregating formula/algorithm;</li>
<li>minimising rounds of communication and computational cost per local device; and/or</li>
<li>enhancing privacy (viz. by adding differential privacy into the mix).</li>
</ol>
</li>
</ul>
<p><span style="font-weight: 400;">Then we will see how said tweaks work in terms of these metrics and report any observed improvements/changes (fingers crossed).</span></p>
<h4>Midway Targets</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Successfully implement an FL model, and come up with some preliminary results;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Suggest some tweaks and improvements to test out;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">If needed, present pertinent concepts from relevant literature.</span></li>
</ul>
<h4>Work Division</h4>
<ol>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Coding/Privacy based considerations - Joint task, will divide up coding of different components fluidly as needed.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Data analysis and visualisation - Suraj</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Mathematics, formulae, proofs - Saswat</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Report writing and project website maintenance - Joint task</span></li>
</ol>
<h4>Expected Results</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Performance of FL based learning: almost as sound as if the learning was centralised, and maybe more nuanced.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Either of these: Increased privacy, less communication rounds with the server, less/quicker local device computation, better aggregation.</span></li>
</ul>
<h4>Relevant Papers</h4>
<ul>
<li aria-level="1"><strong>McMahan et al - </strong><em><span style="font-weight: 400;">Communication-Efficient Learning of Deep Networks from Decentralized Data</span></em></li>
<li aria-level="1"><strong>Konečn&yacute; et al - </strong><em><span style="font-weight: 400;">Practical Secure Aggregation for Privacy Preserving Machine Learning</span></em></li>
<li aria-level="1"><strong>Konečn&yacute; et al - </strong><em><span style="font-weight: 400;">Federated Learning: Strategies for Improving Communication Efficiency</span></em></li>
<li><strong>Rodriguez-Barroso et al - </strong><em><span style="font-weight: 400;">Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy</span></em></li>
</ul>
<hr />
<h4>Project Presentation Slides</h4>
<p><embed style="display: block; margin-left: auto; margin-right: auto;" src="https://drive.google.com/viewerng/viewer?embedded=true&amp;url=https://github.com/SaswatD27/CS460-ML-Project-2021/raw/main/CS460%20-%20Project%20Proposal.pdf" width="500" height="375"></embed></p>
<hr/>
</div>
</html>
