<html>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>

    body {
      margin:0;
      padding:0;
      font-family: "Helvetica Neue", "Segoe UI", -apple-system, BlinkMacSystemFont, Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
      background-color:white;
    }

    .plot {
      width: 100%;
      height: 100%;
    }

    .top-container {
      background-color: #f1f1f1;
      padding: 30px;
      margin: 0%;
    }

    .header {
      padding: 10px 16px;
      background: rgb(0, 0, 0);
      text-align: center;
      color: #f1f1f1;
      border:0%
    }

    .content {
      padding: 16px;
      margin: 15%;
      margin-top: 0%;
    }

    .sticky {
      position: fixed;
      top: 0;
      width: 100%;
    }

    .sticky + .content {
      padding-top: 102px;
      margin: 15%;
      margin-top: 0%;
    }

    /* Three image containers (use 25% for four, and 50% for two, etc) */
    .column {
    float: left;
    width: 33.33%;
    padding: 5px;
    margin-left: 8%;
    }

    /* Clear floats after image containers */
    .row::after {
    content: "";
    clear: both;
    display: table;
    }

    img{
    display: block;
    margin-left: auto;
    margin-right: auto;
    }

    #timeline {
      font-family: Arial, Helvetica, sans-serif;
      border-collapse: collapse;
      width: 100%;
    }

    #timeline td, #timeline th {
      border: 1px solid #ddd;
      padding: 8px;
    }

    #timeline tr:nth-child(even){background-color: #f2f2f2;}

    #timeline tr:hover {background-color: #ddd;}

    #timeline th {
      padding-top: 12px;
      padding-bottom: 12px;
      text-align: left;
      background-color:black;
      color: white;
    }
    </style>
</head>
<div>
<div>
<div class="container">&nbsp;</div>
</div>
</div>
<div>
<div>
<div>
<div class="container"><header>
<h1 style="text-align: center;">Course Project, CS460, Fall 2021-22</h1>
<h2 style="text-align: center;">On Federated Learning and Possible Improvements</h2>
</header>
<p style="text-align: center;">Saswat Das <br />1811138 | saswat.das@niser.ac.in | School of Mathematical Sciences, NISER, HBNI <br />Suraj Ku. Patel<br />1811163 | suraj.kpatel@niser.ac.in | School of Physical Sciences, NISER, HBNI <br /><br /><a href="https://github.com/SaswatD27/CS460-ML-Project-2021">Project Github Repository</a></p>
<hr />
<h3>1. Project Proposal (6 Sept 2021)</h3>
<h4>Introduction</h4>
<img style="display: block; margin-left: auto; margin-right: auto;" src="https://blog.ml.cmu.edu/wp-content/uploads/2019/11/Screen-Shot-2019-11-12-at-10.41.38-AM-970x377.png" alt="Federated Learning Schematic" width="500" />
<p>&nbsp;</p>
<p><em>Federated learning</em> (abbreviated as <em>FL</em>) refers to the practice of conducting machine learning based on several users' data by having each user, given an initial global model by a central server, <em>locally train&nbsp;</em>a model on their local devices, then communicate their locally trained model to the central server, following which each user's local model is&nbsp;<em>aggregated&nbsp;</em>at the central server with that of others, and used to update the global model.</p>
<p>This is in contrast to centralised learning, where the users' data would have been sent to the central server, and a model would have been trained on all that collected data in one central server.</p>
<h5>Advantages?</h5>
<p>Quite some!</p>
<ol>
<li>The user's personal data never leaves their local device, thus ensuring a healthy measure of privacy;</li>
<li>Can possibly incorporate nuances of each user's local model by a suitable aggregation technique/algorithm to provide better, more nuanced learning.</li>
</ol>
<h5>Concerns/Areas of Possible Improvement</h5>
<ul>
<li>Communication costs post local training and optimising number of communication rounds between each local device and the central server:</li>
<li>Cost of computation due to local training on each device and effects on user experience during local training;</li>
<li>Possible improvements in aggregation algorithms;</li>
<li>Enhancement of privacy, and making sure no adversary can conclude from the (say, weights of the) local models an individual user's personal information or behaviour.</li>
</ul>
<h4>What we intend to do - a brief sketch</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Implement a Federated Learning algorithm (tentatively </span><span style="font-weight: 400;">FederatedAveraging</span><span style="font-weight: 400;">) after locally training models using algorithms like SGD (as suggested) on generated datasets, or even implementing some neural network(s) if possible using popular datasets (viz. FEMNIST, Shakespeare, Sentiment140, CIFAR 10) if we are feeling bold. We can use TensorFlow and SherpaAI for this if possible/needed (no promises).</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Using the (local) learning algorithm as a baseline, we can compare the results of our FL based trained-and-aggregated model with centralised learning with the same algorithm, and some other popular algorithms.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;"><span style="font-weight: 400;">Suggest improvements to the paradigm just implemented in terms of</span></span>
<ol>
<li>The aggregating formula/algorithm;</li>
<li>minimising rounds of communication and computational cost per local device; and/or</li>
<li>enhancing privacy (viz. by adding differential privacy into the mix).</li>
</ol>
</li>
</ul>
<p><span style="font-weight: 400;">Then we will see how said tweaks work in terms of these metrics and report any observed improvements/changes (fingers crossed).</span></p>
<h4>Midway Targets</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Successfully implement an FL model, and come up with some preliminary results;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Suggest some tweaks and improvements to test out;</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">If needed, present pertinent concepts from relevant literature.</span></li>
</ul>
<h4>Work Division</h4>
<ol>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Coding/Privacy based considerations - Joint task, will divide up coding of different components fluidly as needed.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Data analysis and visualisation - Suraj</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Mathematics, formulae, proofs - Saswat</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Report writing and project website maintenance - Joint task</span></li>
</ol>
<h4>Expected Results</h4>
<ul>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Performance of FL based learning: almost as sound as if the learning was centralised, and maybe more nuanced.</span></li>
<li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Either of these: Increased privacy, less communication rounds with the server, less/quicker local device computation, better aggregation.</span></li>
</ul>
<h4>Relevant Papers</h4>
<ul>
<li aria-level="1">McMahan, H. B. et al. &ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data.&rdquo; AISTATS (2017).</li>
<li aria-level="1">Konecn&yacute;, Jakub et al. &ldquo;Federated Learning: Strategies for Improving Communication Efficiency.&rdquo; ArXiv abs/1610.05492 (2016): n. pag.</li>
<li>Barroso, Nuria Rodr&iacute;guez et al. &ldquo;Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy.&rdquo; Inf. Fusion 64 (2020): 270-292.</li>
</ul>
<hr />
<h4>Project Presentation Slides</h4>
<embed style="background-image: url('img/object.gif'); display: block; margin-left: auto; margin-right: auto;" src="https://drive.google.com/viewerng/viewer?embedded=true&amp;url=https://github.com/SaswatD27/CS460-ML-Project-2021/raw/main/CS460%20-%20Project%20Proposal.pdf" width="500" height="375"></embed><hr /></div>
</div>
</div>
</div>
<div>
<div>
<h3>2. Project Midway</h3>
</div>
</div>
<div>
<div>
<h4>Literature Review</h4>
</div>
</div>
<div>
<div>
<p>Some of the most important papers we referred to are listed below.</p>
</div>
</div>
<div>
<div>
<ul>
<li>McMahan, H. Brendan et al. &ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data.&rdquo;&nbsp;<em>AISTATS</em>&nbsp;(2017).</li>
<li>Konecn&yacute;, Jakub et al. &ldquo;Federated Learning: Strategies for Improving Communication Efficiency.&rdquo;&nbsp;<em>ArXiv</em> abs/1610.05492 (2016).</li>
<li>Wei, Kang et al. &ldquo;Federated Learning With Differential Privacy: Algorithms and Performance Analysis.&rdquo;&nbsp;<em>IEEE Transactions on Information Forensics and Security</em>&nbsp;15 (2020): 3454-3469.</li>
<li>Ji, Shaoxiong et al. &ldquo;Dynamic Sampling and Selective Masking for Communication-Efficient Federated Learning.&rdquo;&nbsp;<em>ArXiv</em> abs/2003.09603 (2020).</li>
</ul>
</div>
</div>
<div>
<div>
<ol>
<li><strong>McMahan et al</strong>
<ul>
<li>
<p>Introduces federated learning;</p>
</li>
<li>
<p>Addresses issues such as unbalanced volume of datapoints across all clients, limited communication capabilities of clients (via multiple rounds of communication), and the mass distributed nature of the clients, and the non-IID nature of data as an individual&rsquo;s data is specific;</p>
</li>
<li>
<p>Introduces <code>FedAvg</code> (<span class="math inline">\(w_{t+1}\gets\sum_{i=1}^K\frac{n_k}{n}w_{t+1}^k\)</span>);</p>
</li>
<li>
<p>Talks about controlling parallelism of local computation and increased local computation by varying batch size for gradient descent (decreasing it increases amount/precision of computation, no. of rounds, and no. of clients queried per round (affects parallelism).</p>
</li>
<li><strong>Remarks:&nbsp;</strong>
<ul>
<li>This provides us with a robust federated learning paradigm, but of course, as we shall soon see, this is a naive approach in terms of privacy, as there have supposedly been attacks in which adversaries have used weight updates in this scheme, say from a facial recognition application, to compromise the identities of users;</li>
<li>Also this may seem pretty intuitive (and as we shall see, this works pretty well for SGD), it is sort of a simple arithmetic mean at the end of the day, and might be unable to capture some subtle nuances of the weight vectors received from clients, maybe some non linear aggregation formula could work better in certain contexts, and it is a good area to look into.</li>
<li>The idea behind increasing parallelism by lowering the number of rounds of communication and increasing computation in between rounds locally by making the task at hand more precise (viz. by reducing batch size for gradient descent) is fairly intuitive, but is there a way to have clients with slower connections begin to upload their weight vectors for a certain (scheduled) round of communication prior to others with no such constraint? If so, how should rounds be scheduled?</li>
</ul>
</li>
</ul>
</li>
<li><strong>Wei et al</strong>
<ul>
<li>
<p>Introduces Gaussian noise w.r.t. a clipping parameter <span class="math inline">\(C\)</span> to weight vector uploads, weights scaled w.r.t. <span class="math inline">\(C\)</span>;</p>
</li>
<li>
<p>Motivation: Na&iuml;vely uploaded weights carry the risk of being used by adversaries to compromise users;</p>
</li>
<li>
<p>Proposes uplink and optional downlink noise addition;</p>
</li>
<li>
<p>Takes fairly large values for the privacy budget;</p>
</li>
<li>
<p>Accuracy improves with no. of clients queried and rounds of communication;</p>
</li>
<li>
<p>Best accuracy when clients have (near) identical amounts of quality data;</p>
</li>
<li>
<p>Akin to central differential privacy, straightforward noise addition.</p>
</li>
<li><strong>Remarks:</strong>
<ul>
<li>This is a fairly straightforward method of adding Gaussian noise (and ergo endow approximate differential privacy) into the weight uploads. But are there better ways of doing the same? We briefly considered implementing something similar using the exponential mechanism, and given how versatile it is, it could perhaps capture the nuances of the local training better, especially if you consider that out-of-bound (w.r.t. the clipping threshold) weight vectors are simply scaled down w.r.t. the clipping threshold (for good reason), can this be made more dynamic and inclusive of the factors and nuances in each client's dataset/model? It is an interesting thought.</li>
<li>We also noticed that the authors do not mention a norm per se in the paper for the weight vectors while defining their algorithm (though there are some obvious choices, this was a bit unsettling).</li>
<li>The authors use large values of the privacy budget (given the number of clients/the massively distributed nature of the client pool, this seems fair); can this be reduced while not having a major increase in error? Or with the minimum possible increase in the number of required clients to counteract any increase in noise?</li>
</ul>
</li>
</ul>
</li>
<li><strong>Konečn&yacute; et al</strong>
<ul>
<li>
<p>Introduces methods to reduce the uplink communication costs by reducing the size of the updated model sent back by the client to the server.</p>
</li>
<li>
<p>Motivation: Poor bandwidth/expensive connections of a number of the participating devices (clients) leads to problems during the aggregation of data for FL.</p>
</li>
<li>
<p>Two methods for sending a smaller model are:</p>
<ul>
<li>
<p><em>Structured updates</em>, updates are from a restricted space and can be parametrized using a smaller number of variables. Algorithms: Low Rank and Random Mask.</p>
</li>
<li>
<p><em>Sketched updates</em>, full model updates are learned but a compressed model update is sent to the server. Algorithms: Subsampling, Probabilistic quantization, and structured random rotations.</p>
</li>
</ul>
</li>
<li>
<p><code>FedAvg</code> is used for the experiments to decrease the number of rounds of communication required to train a good model.</p>
</li>
<li>
<p>Conclusions of the paper</p>
<ul>
<li>
<p>Random mask performs significantly better than low rank, as the size of the updates is reduced.</p>
</li>
<li>
<p>Random masking gives higher accuracy as compared to sketched updates method but reaches moderate accuracy much slower.</p>
</li>
<li>
<p>Quantization algorithm alone is very unstable For a small number of quantization bits and smaller modes, and random rotation with quantization is more stable and has improved performance as compared to without rotation.</p>
</li>
<li>
<p>By increasing the number of rounds of training, the fraction of clients taken per round can be minimised without risking accuracy.</p>
</li>
<li>
<p>Introduced an important and practical tradeoff in the FL: one can select more clients in each round while having each of them communicate less, and obtain the same accuracy as using fewer clients, but having each of them communicate more.</p>
</li>
<li>Note (The random mask's indices/low rank's encoding matrix can be elegantly encoded in a random seed, we haven't implemented it that way per se, though the idea is the same, but that is a fairly elegant manner of going about it, not to mention the significant advantage of terms of just having to remember a seed and not an array of indices/a matrix.)</li>
</ul>
</li>
</ul>
</li>
<li>Extra:<strong> Ji et al</strong>
<ul>
<li>Proposes dynamic sampling and selective mask;</li>
<li>Dynamic sampling: In contrast to the usual way of sampling clients during each round of communication (i.e. static sampling), we define a decay constant, called <span class="math inline">\(\beta\), and if the initial sampling rate is <span class="math inline">\(C\)</span>, then we choose <span class="math inline">\(\frac{C}{\exp(\beta t)}\)</span> fraction of clients for round <span class="math inline">\(t\)</span> (starting with <span class="math inline">\(t=0\)</span>)</span>;</li>
<li>Selective mask: Instead of fully (uniformly) randomised selection of indices for a mask, choose the top <span class="math inline">\(k\)</span> indices with the most significant updates to the initial model for each client;</li>
<li>Selective mask consistently outperformed random mask in terms of accuracy and rate of convergence;</li>
<li>Dynamic sampling gave accuracy comparable to static sampling, but at a significant loss in communication cost.</li>
<li><strong>Remarks:</strong>
<ul>
<li>We have a few bones to pick here: while these work extremely well all things considered, would not it be better if dynamic sampling chose to sample a larger number of clients in response to an increase in observed error after a particular vis-a-vis previous rounds to help decrease the error for subsequent rounds? This could be a neat feature.</li>
<li>Selective sampling chooses a specified number of the most significant (by magnitude) updates, and it definitely seems to be working really well, but we are concerned if in the process, i.e. by disregarding the others, we might be losing some important nuance in the process (i.e. this reasoning seems to be kind of reductionist and black-and-white). So can we instead choose mostly from among the top updates, and reserve a small proportion for those not in that club, so to speak? And if so, will this approach yield any advantages? Might as well try that out.&nbsp;</li>
</ul>
</li>
</ul>
</li>
</ol>
</div>
</div>
<div>
<div>
<h4>Techniques Used/Explored</h4>
</div>
</div>
<div>
<div>
<ul>
<li>
<p>For aggregation, we used <code>FedAvg </code>for the most vanilla FL implementation, and then added to it as necessary for the implementation of more sophisticated FL techniques;</p>
</li>
<li>
<p>The Gaussian Mechanism to provide <span class="math inline">\((\varepsilon,\delta)\)</span>-differential privacy pre-upload from each device (Noising before Aggregation), as introduced by Wei et al.;</p>
</li>
<li>
<p><em>Static Sampling</em> of clients for every round of training to reduce communication rounds per user;</p>
</li>
<li>
<p><em>Random Mask</em> and <em>Probabilistic Quantisation</em> as described by Konečn&yacute; et al;</p>
</li>
<li>
<p><em>Dynamic Sampling</em> of clients to successively reduce the proportion of clients involved in successive rounds of communication to further save on rounds of communication and computational resources (as proposed by Ji et al);</p>
</li>
<li>
<p>Considering/considered using: <em>Selective Mask</em>&nbsp;(especially when working with larger weight vectors/matrices).</p>
</li>
</ul>
</div>
</div>
<div>
<div>
<h4>Experiments and Observations</h4>
</div>
</div>
<div>
<div>
<p>We implemented <code>FedAvg</code> from scratch in Python, with a 0.25 static sampling rate, with anywhere between 3 to 5 (or more) rounds of calling for training from sampled (w.r.t. the uniform distribution) clients, with local training involving multiple linear regression implemented via Stochastic Gradient Descent on a synthetically generated dataset for about 500 clients, with static subsampling (contrast with dynamic subsampling).</p>
</div>
</div>
<div>
<div>
<p>The generated datasets, generated around a pre-chosen/generated "true weights vector" (we chose the dimension of these weight vectors to be 7) (e.g. <span class="math inline">\([1, 2, 4, 3, 5, 6, 1.2]\)</span>), are mostly IID as we focused more on implementing a model atop that.</p>
</div>
</div>
<div>
<div>
<p>As we are using regression here, we use, for a particular instance, average training/testing error per data point as a metric of a deployment&rsquo;s accuracy, and simply measure the time taken by running it on Google Colab and compare them as a rough measure of how quick each is, and implement centralised learning to serve as a baseline for our exploration of these FL paradigms.</p>
</div>
</div>
<div>
<div>
<p>We then tried the above listed techniques, sometimes standalone or in conjunction with each other as follows.</p>
</div>
</div>
<div>
<div>
<ol>
<li><strong>Centralised Learning (As a Baseline)</strong><br />
<p>For centralised learning, we simply gathered and flattened the list of all local datasets into a cumulative list of all datapoints, and ran SGD on it for 100 epochs.<br />Time Taken <span class="math inline">\(\approx 268-270\)</span> seconds.<br />Average Training Error for Centralised Learning on SGD <span class="math inline">\(\approx 3.2472\times 10^{-29}\).</span><br />Average Testing Error for Centralised Learning on SGD <span class="math inline">\(\approx 3.2154 \times 10^{-29}\).</span></p>
</li>
<li><strong>Vanilla <code>FederatedAveraging/FedAvg</code></strong>
<p>We then ran vanilla <code>FedAvg</code> with static sampling of clients at a rate of 0.25 of the client population per round of training on the above mentioned local datasets, with 100 epochs per round of local training. Number of rounds, <span class="math inline">\(T=5\)</span>.<br />Time taken <span class="math inline">\(\approx 362\)</span> seconds.<br />Average Training Error for Vanilla FedAvg on SGD <span class="math inline">\(\approx 2.5331\times 10^{-18}\)</span>.<br />Average Testing Error for Vanilla FedAvg on SGD <span class="math inline">\(\approx 2.6021\times 10^{-18}\)</span>.</p>
</li>
<li><strong>Noising before Aggregation (From Wei et al)</strong>
<p>Adds Gaussian noise to appropriately clipped weights from each user. Motivation: To stop attacks involving reconstruction of raw data/private information from na&iuml;ve uploading of weight vectors/matrices. Optionally adds downlink DP, but we felt it was unnecessary.</p>
<p>We an <code>FedAvg</code> with static sampling of clients at a rate of 0.25 per round of training, clipping the locally generated weight vectors to gain bounds (defining upper bound of the norm of a weight vector as <span class="math inline">\(C =1.01\times\max(w_i)\)</span>, where <span class="math inline">\(w -\)</span> weight vector, for the sake of computing the sensitivity of queries for the weight vectors, then adding Gaussian noise calibrated to <span class="math inline">\(C,\varepsilon,\delta\)</span> to each weight (<span class="math inline">\(\varepsilon=70,\delta=0.1\)</span>) with 100 epochs per round of local training. Number of rounds, <span class="math inline">\(T=5\)</span>.<br />Time taken <span class="math inline">\(\approx 349\)</span> seconds (which makes it about as fast as vanilla <code>FedAvg</code>).<br />Average Training Error for NbA FedAvg on SGD <span class="math inline">\(\approx 7.2562\times 10^{-7}\)</span>.<br />Average Testing Error for NbA FedAvg on SGD <span class="math inline">\(\approx 6.6740\times10^{-7}\)</span>.</p>
</li>
<li><strong>NbAFL with Random Mask</strong>
<p>We use the same setup as for the above NbAFL implementation, but with a layer of uniformly chosen random masks, excluding <span class="math inline">\(\approx s=0.25\)</span> of the weights, prior to uploading by a client. Seems to consistently outperform base NbAFL in terms of accuracy, also reduces communication overhead!<br />Time taken <span class="math inline">\(\approx 356\)</span> seconds.<br />Average Training Error <span class="math inline">\(\approx 0.006601\).</span><br />Average Testing Error <span class="math inline">\(\approx 0.007577\).</span></p>
</li>
<li><strong>NbAFL with Probabilistic Binarisation</strong>
<p>We again use the same setup and parameters as our vanilla NbAFL implementation, but with probabilistic binarisation implemented atop it.<br />Time taken <span class="math inline">\(\approx 356\)</span> seconds.<br />Average Training Error <span class="math inline">\(\approx 0.021241\)</span>.<br />Average Testing Error <span class="math inline">\(\approx 0.019635\)</span></p>
</li>
<li><strong>NbAFL with Dynamic Sampling</strong>
<p>We again adopt the same setup as that for our vanilla NbAFL implementation, but instead of static sampling of clients, we sample clients with an initial rate of <span class="math inline">\(0.25\)</span>, which decays by a factor of <span class="math inline">\(\frac 1{\exp(\beta t)}\)</span>, where <span class="math inline">\(t =\)</span> number of rounds elapsed. We take <span class="math inline">\(\beta=0.05\)</span>.<br />Time taken <span class="math inline">\(\approx 334\)</span> seconds.<br />(Saves on time and no. of communication rounds!)<br />Average Training Error <span class="math inline">\(\approx 0.010684\)</span>.<br />Average Testing Error <span class="math inline">\(\approx 0.010908\)</span>.</p>
</li>
</ol>
</div>
</div>
<div>
<div>
<p>The following table summarises the parameters used for and the results of our experiments (to be fair, we had to fiddle with a fair of different values for parameters and variations in algorithms to fix these for now)</p>
</div>
</div>
<div>
<div>
<div id="tab:my-table">
<table style="margin-left: auto; margin-right: auto; height: 353px;"><caption>Local Learning Rate, <span class="math inline">\(\alpha = 0.01\)</span>; No. of local iterations for SGD <span class="math inline">\(=100\)</span>, No. of clients <span class="math inline">\(=500\)</span></caption>
<thead>
<tr class="header" style="height: 53px;">
<th style="text-align: left; height: 53px; width: 78.6806px;">Name of Model</th>
<th style="text-align: left; height: 53px; width: 9.55556px;">T</th>
<th style="text-align: left; height: 53px; width: 73.2361px;">Client Sampling Rate</th>
<th style="text-align: left; height: 53px; width: 116.486px;"><span class="math inline">\(\varepsilon\)</span></th>
<th style="text-align: left; height: 53px; width: 64.4722px;"><span class="math inline">\(delta\)</span></th>
<th style="text-align: left; height: 53px; width: 100.931px;">Training Error</th>
<th style="text-align: left; height: 53px; width: 175.931px;">Testing Error</th>
<th style="text-align: left; height: 53px; width: 47.5694px;">Time Taken (sec)</th>
<th style="text-align: left; height: 53px; width: 99.6806px;">Other Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd" style="height: 53px;">
<td style="text-align: left; height: 53px; width: 78.6806px;">Centralised SGD</td>
<td style="text-align: center; height: 53px; width: 9.55556px;">-</td>
<td style="text-align: center; height: 53px; width: 73.2361px;">-</td>
<td style="text-align: center; height: 53px; width: 116.486px;">-</td>
<td style="text-align: center; height: 53px; width: 64.4722px;">-</td>
<td style="text-align: center; height: 53px; width: 100.931px;"><span class="math inline">\(3.2472\times 10^{-29}\)</span></td>
<td style="text-align: center; height: 53px; width: 175.931px;"><span class="math inline">\(3.2154 \times 10^{-29}\)</span></td>
<td style="text-align: center; height: 53px; width: 47.5694px;">268 - 270</td>
<td style="text-align: center; height: 53px; width: 99.6806px;">-</td>
</tr>
<tr class="even" style="height: 53px;">
<td style="text-align: left; height: 53px; width: 78.6806px;">Vanilla <code>FedAvg</code></td>
<td style="text-align: center; height: 53px; width: 9.55556px;">5</td>
<td style="text-align: center; height: 53px; width: 73.2361px;">0.25</td>
<td style="text-align: center; height: 53px; width: 116.486px;">-</td>
<td style="text-align: center; height: 53px; width: 64.4722px;">-</td>
<td style="text-align: center; height: 53px; width: 100.931px;"><span class="math inline">\(2.5331\times 10^{-18}\)</span></td>
<td style="text-align: center; height: 53px; width: 175.931px;"><span class="math inline">\(2.6021\times 10^{-18}\)</span></td>
<td style="text-align: center; height: 53px; width: 47.5694px;">362</td>
<td style="text-align: center; height: 53px; width: 99.6806px;">-</td>
</tr>
<tr class="odd" style="height: 53px;">
<td style="text-align: left; height: 53px; width: 78.6806px;">NbAFL</td>
<td style="text-align: center; height: 53px; width: 9.55556px;">5</td>
<td style="text-align: center; height: 53px; width: 73.2361px;">0.25</td>
<td style="text-align: center; height: 53px; width: 116.486px;">70</td>
<td style="text-align: center; height: 53px; width: 64.4722px;">0.01</td>
<td style="text-align: center; height: 53px; width: 100.931px;"><span class="math inline">\(7.2562\times 10^{-7}\)</span></td>
<td style="text-align: center; height: 53px; width: 175.931px;"><span class="math inline">\(6.6740\times10^{-7}\)</span></td>
<td style="text-align: center; height: 53px; width: 47.5694px;">349</td>
<td style="text-align: center; height: 53px; width: 99.6806px;">-</td>
</tr>
<tr class="even" style="height: 53px;">
<td style="text-align: center; height: 53px; width: 78.6806px;">NbAFL w/ Random Mask</td>
<td style="text-align: center; height: 53px; width: 9.55556px;">5</td>
<td style="text-align: center; height: 53px; width: 73.2361px;">0.25</td>
<td style="text-align: center; height: 53px; width: 116.486px;">70</td>
<td style="text-align: center; height: 53px; width: 64.4722px;">0.01</td>
<td style="text-align: center; height: 53px; width: 100.931px;">0.006601</td>
<td style="text-align: center; height: 53px; width: 175.931px;">0.007577</td>
<td style="text-align: center; height: 53px; width: 47.5694px;">356</td>
<td style="text-align: center; height: 53px; width: 99.6806px;"><span class="math inline">\(s=0.25\)</span></td>
</tr>
<tr class="odd" style="height: 35px;">
<td style="text-align: center; height: 35px; width: 78.6806px;">NbAFL w/ Prob. Bin.</td>
<td style="text-align: center; height: 35px; width: 9.55556px;">5</td>
<td style="text-align: center; height: 35px; width: 73.2361px;">0.25</td>
<td style="text-align: center; height: 35px; width: 116.486px;">70</td>
<td style="text-align: center; height: 35px; width: 64.4722px;">0.01</td>
<td style="text-align: center; height: 35px; width: 100.931px;">0.021241</td>
<td style="text-align: center; height: 35px; width: 175.931px;">0.019635</td>
<td style="text-align: center; height: 35px; width: 47.5694px;">356</td>
<td style="text-align: center; height: 35px; width: 99.6806px;">-</td>
</tr>
<tr class="even" style="height: 53px;">
<td style="text-align: center; height: 53px; width: 78.6806px;">NbAFL w/ Dyn. Samp.</td>
<td style="text-align: center; height: 53px; width: 9.55556px;">5</td>
<td style="text-align: center; height: 53px; width: 73.2361px;">0.25</td>
<td style="text-align: center; height: 53px; width: 116.486px;">70</td>
<td style="text-align: center; height: 53px; width: 64.4722px;">0.01</td>
<td style="text-align: center; height: 53px; width: 100.931px;">0.010684</td>
<td style="text-align: center; height: 53px; width: 175.931px;">0.010908</td>
<td style="height: 53px; width: 47.5694px; text-align: center;">334</td>
<td style="text-align: center; height: 53px; width: 99.6806px;"><span class="math inline">\(\beta=0.05\)</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div>
<div>
<p>Note that the errors happen to be quite small compared to the weight vectors' magnitude, with a maximum being in the neighbourhood of 1% error to "true" weight vector (norm/coordinates) ratio, roughly.</p>
</div>
</div>
<div>
<div>
<h4>Suggested Tweaks and Possible Improvements to Try Out to Extend the Baselines</h4>
</div>
</div>
<div>
<div>
<ul>
<li>
<p>Silo-ing users with similar characteristics and running a separate FL paradigm within these silos;</p>
</li>
<li>
<p>Applications to dapps (decentralised applications) on P2P networks (given that uploaded weight vectors are more or less public and aggregation does not take much time);</p>
</li>
<li>
<p>Trying other mechanisms (viz. exponential) out on NbAFL for possible improvement, or explore locally differentially private techniques instead (some baseline work on this exists);</p>
</li>
<li>
<p>Trying Selective Mask from out, and slightly diffusing the choice from the top <span class="math inline">\(k\)</span> updates to some of the lower updates;</p>
</li>
<li>
<p>Making dynamic sample more dynamic and efficient by calibrating decay w.r.t. error per round of communication (more error<span class="math inline">\(\implies\)</span>less decay;</p>
</li>
<li>
<p>Exploring mutual benefits of probabilistic quantisation and noise addition, calibrating no. of quanta to the magnitude of the noise;</p>
</li>
<li>
<p>Looking at tweaks to <code>FedAvg</code> in certain contexts;</p>
</li>
<li>Think of how to further unlink clients from their uploaded weights (perhaps mixes will help);</li>
<li>
<p>Anything else that strikes our minds.</p>
</li>
</ul>
</div>
</div>
<div>
<div>
<p>We may tentatively try implementing the above FL models for more complex local training algorithms (viz. ConvNets using FEMNIST), but this is secondary/optional.</p>
</div>
</div>
<div><hr /></div>
<div>
<p>Here is the <a href="https://github.com/SaswatD27/21cs460_group01/blob/main/Code/CS460_FL_1_01.ipynb">link</a> to the .ipynb file containing the latest version of our code (as of 17 Oct '21).</p>
</div>
<div>
<div><hr /></div>
</div>
<div>
<div>
<h4>Midway Presentation Slides</h4>
(Kindly Exclude The Title and Bibliography Slides from the Slide Count)</div>
</div>
<div>
<div>
<p><embed style="background-image: url('img/object.gif'); display: block; margin-left: auto; margin-right: auto;" src="https://drive.google.com/viewerng/viewer?embedded=true&amp;url=https://github.com/SaswatD27/21cs460_group01/raw/main/CS460_Team_01_Midway_Presentation.pdf" width="500" height="375"></embed></p>
</div>
</div>
<div>
<div><hr /></div>
</div>
<div>
<div>&nbsp;</div>
</div>
<div>
<div><hr />
<h3>3. Final Project Presentation/Extension of Baselines</h3>
<p>&nbsp;</p>
</div>
</div>
<ul>Since the midway, we did a number of things, the most notable of which are as follows.
<li>
<p>Created a federated learning code framework from scratch to transfer our work from one that employs local learning via linear regression on a randomly generated dataset to one that employs DNNs (implemented on Tensorflow) to classify digits in MNIST (randomly shuffled and distributed across clients).</p>
</li>
<li>
<p>Tried out various approaches and tweaks to some existing paradigms/models.</p>
</li>
<li>
<p>Designed "<strong>Adaptive Sampling</strong>" an improvement to Dynamic Sampling, with penalisation of the sampling decay coefficient for increase in errors.</p>
</li>
<li>
<p>Designed a <strong>fully decentralised P2P approach</strong> to FL (in contrast to the recent P2P models proposed that involve a level of temporary centralisation).</p>
</li>
<li>
<p>Designed an approach involving <strong>clustering (of clients) in silos</strong> and simultaneously training a <em>silo-specific model</em> and a <em>general model</em> via FL.</p>
</li>
</ul>
<p>&nbsp;</p>
<h4>Specifications of our new framework</h4>
<p><strong>Dataset:</strong></p>
<dl>
<dd>
<p><strong>MNIST</strong> (training data shuffled and distributed unequally at random among 100 clients)</p>
</dd>
<dt><strong>Local Training:</strong></dt>
<dd>
<p>Via a <strong>DNN</strong>, defined and compiled with Tensorflow.</p>
<ul>
<li>
<p><strong>Input Layer</strong>: Flatten Layer.</p>
</li>
<li>
<p><strong>Hidden Layers</strong>: 32 units and 512 units* (later ditched for speed) with ReLU activation.</p>
</li>
<li>
<p><strong>Dropout Layer*</strong> (rate = 0.2): Before the <span class="math inline">\(2^\text{nd}\)</span> Hidden Layer (later ditched)</p>
</li>
<li>
<p><strong>Output Layer:</strong> 10 units with Softmax activation.</p>
</li>
<li>
<p><strong>Batch Normalisation:</strong> Before every hidden and output layer</p>
</li>
</ul>
</dd>
</dl>
<p>In a nutshell, due to time and computing power constraints, we ended up using a DNN with a flatten layer, a hidden layer with 32 units with ReLU activation, followed by an output layer with 10 units with softmax activation, not to mention the batch normalisation layers before the hidden and output layer. As we shall soon see, we found that this was sufficient to give a high degree of accuracy on MNIST.</p>
<dl>
<dt>Federated Averaging was implemented similarly as for our older framework, as per the original paper on Federated Averaging by McMahan et al.</dt>
</dl>
<p>A salient feature of our framework is that it is flexible enough to be used for any Tensorflow based neural network (at least ones that are created using keras.sequential) and for any dataset that is fed into such an NN.</p>
<h4>Baselines</h4>
<p><strong>N.B.</strong> For testing, we initialised the same server model (i.e. we started training with a weight vector that was common for every run, for example, a weight vector with all 0s for each "coordinate" in the vector) for each run of each model, for a fair comparison.</p>
<p>&nbsp;</p>
<p>Given the conception of the new code framework with the DNN on MNIST, we re-ran some of the relevant baselines using it, and we got the following results. Please note that the provided figures/graphs are representative, as in they are just a few from the many runs of each model/algorithm.</p>
<p><strong>Centralised Learning</strong><br />Which simply is training the DNN using all of the MNIST training data in one place, and precisely what any federated learning algorithm should aspire to match, or at least approach, in terms of accuracy.<br /><strong>Test Accuracy:</strong> 96.52% (Pretty nice!)</p>
<p><img src="https://github.com/SaswatD27/21cs460_group01/raw/main/images/MNIST%20Central%201.png" alt="image" /></p>
<p><strong>Vanilla FedAveraging</strong></p>
<p>Which, as mentioned earlier, is simply an implementation of McMahan et al's model with aggregation via FederatedAvg. Note that here in each round of communication, a fixed proportion of the total number of clients are asked for local updates, which in the language of client sampling is called static sampling.<br /><strong>Number of Communication Rounds:</strong> 50<br /><strong>Test Accuracy:</strong> 94.21% (Not bad at all!)</p>
<p><img src="https://github.com/SaswatD27/21cs460_group01/raw/main/images/MNIST%2050%20FedAvg.png" alt="image" /></p>
<p><strong>FedAveraging with Dynamic Sampling</strong></p>
<p>For a proper recaptitulation of what Dynamic Sampling does, refer to our prior discussion regarding Ji et al in the Midway section. But in a nutshell, what it does is reduce the number of clients sampled per round w.r.t. a decay coefficient <span class="math inline">\(\beta\)</span> and the initial client sampling rate (i.e. by multiplying <span class="math inline">\(\frac 1{\exp(-\beta t)}\)</span> to the initial sampling rate).</p>
<p><strong>Decay Coeff. <span class="math inline">\(\beta\)</span>:</strong> 0.05<br /><strong>Initial Sampling Rate :</strong> 0.25<br /><strong>Test Accuracy:</strong> 93.7%<br /><strong>Number of Rounds:</strong> 50</p>
<p><img src="https://github.com/SaswatD27/21cs460_group01/raw/main/images/MNIST%20Dyn%201%2020.png" alt="image" /></p>
<p>Note that dynamic sampling decreases the number of clients per round independent of the change of accuracy per round.<br />The problem? This can slow down convergence and error rectification, or even occasionally and briefly lead to consecutive increases in error, because aggregation of several client weights leads to a good aggregate weights estimate, but instead the number of clients is continually decreasing here with the naive approach that decreasing the number of clients per round would decrease volume of communication. Okay, that is true, but especially in a short term manner. In the long term, slow error rectification will lead to the model requiring larger number of rounds of communication, which incurs both a time cost and communication cost in that way. But we can see that the thinking behind this paradigm is not without merit, we got a really good accuracy for substantially less clients' involvement vis-a-vis the vanilla case. But can we couple the reduction of the number of clients with faster convergence in a way that we are fine having the initial decay coefficient (which is positively correlated to how fast the decay occurs) as long as the error does not increase beyond a previously attained error value, at which point we would want to slow down the decay and make sure to involve more clients than in the last round to mitigate the amount of error and bring the amount of accuracy eventually to the highest previously attained accuracy beyond which it fell off. This is precisely the motivation behind us introducing <strong>adaptive sampling</strong>.</p>
<p>More clients involved<span class="math inline">\(\implies\)</span>Better averaging<span class="math inline">\(\implies\)</span>Less error.</p>
<p><br /><strong>Adaptive sampling<br /></strong>That is, we keep penalising decay coefficient for fall in accuracy, while retaining the previous index, so as to increase the number of clients for the next round and slow the decay in the sampling rate down, and restore the decay coefficient when accuracy reaches the previous high.</p>
<p>The pseudocode for adaptive sampling is as follows.</p>
<p><img style="display: block; margin-left: auto; margin-right: auto;" src="https://github.com/SaswatD27/21cs460_group01/raw/main/images/Adaptive%20Sampling%20Algo.jpg" alt="image" width="540" height="628" /></p>
<p>&nbsp;</p>
<p><strong>Note :</strong> <span class="math inline">\(n_i\)</span> is the number of datapoints possessed by the client corresponding to <span class="math inline">\(\Omega_t^i\)</span>, and <span class="math inline">\(n\)</span> is the total of all <span class="math inline">\(n_i\)</span>&rsquo;s for each <span class="math inline">\(\Omega_t^i\in L\)</span>.</p>
<p>Also note that <span class="math inline">\((\gamma)\) is just a parameter we introduced in case we needed to tune the penalisation of the decay coefficient, and we kept it equal to 1 to start off with, and as it turned out, that worked just fine, so we kept it at 1.</span></p>
<p>We again ran several rounds of experiments with for adaptive sampling and a representative results, with similar parameters as for the previous representative run for dynamic sampling, are as follows:</p>
<p><strong>Initial Decay Coeff. <span class="math inline">\(\beta\)</span>:</strong> 0.05<br /><strong>Initial Sampling Rate :</strong> 0.25<br /><strong>Gamma <span class="math inline">\((\gamma)\)</span> :</strong> 1<br /><strong>Test Accuracy:</strong> 94.16%<br /><strong>Rounds of Communication:</strong> 20</p>
<p>(It would have risen as accuracy fell in the last step; it had an accuracy of 94.59% after round 19, but since we only specified 20 rounds of communication, it stalled at that.)</p>
<p><img style="float: left;" src="https://github.com/SaswatD27/21cs460_group01/raw/main/images/MNIST%20Adap%201%2020.png" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>What immediately jumps out at us upon viewing this graph is that 1. it seems to lead to smoother convergence, sans any of the jaggedness as for dynamic sampling; and 2. it leads to convergence within lesser rounds of communication, albeit with marginally more clients per round on average being involved than dynamic sampling (owing to the penalisation of the decay coefficient at times along with stalling of the counter variable whenever accuracy drops), and knowing/assuming that the server calls on clients for communication round wise and clients in each round upload their local weights nearly simultaneously, lesser number of rounds implies lesser time taken for this entire exercise of federated learning.</p>
