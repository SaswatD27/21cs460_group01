{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subclustering_P2P.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaswatD27/21cs460_group01/blob/main/Code/Subclustering_P2P_OldFramework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLp_IZkhWIYV"
      },
      "source": [
        "## Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O977Nz9QsD_T"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_qaNb_lzwPi"
      },
      "source": [
        "# Data Handler\n",
        "\n",
        "### Handle data for training in device and testing in server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arijAkh2sIbf"
      },
      "source": [
        "# Generates three set of data with linear trend\n",
        "def get_data(m, n): \n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for i in range(n):\n",
        "        x = np.random.rand(w_n)\n",
        "        y = np.dot(m, x)\n",
        "        x_cord.append(x)\n",
        "        y_cord.append(y)\n",
        "    return x_cord, y_cord\n",
        "\n",
        "def data_1():\n",
        "    m = m1\n",
        "    # print(m)\n",
        "    x, y = get_data(m, sum(n_k_dev))\n",
        "    i = 0\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for n0 in n_k_dev:\n",
        "        x_cord.append(x[i:i + n0])\n",
        "        y_cord.append(y[i:i + n0])\n",
        "        i += n0\n",
        "    return x_cord, y_cord\n",
        "\n",
        "\n",
        "def test_data1():\n",
        "    m = m1\n",
        "    testing_x, testing_y = get_data(m, 100)\n",
        "    return testing_x, testing_y\n",
        "\n",
        "################################################################################\n",
        "def data_2():\n",
        "    m = m2\n",
        "    # print(m)\n",
        "    x, y = get_data(m, sum(n_k_dev))\n",
        "    i = 0\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for n0 in n_k_dev:\n",
        "        x_cord.append(x[i:i + n0])\n",
        "        y_cord.append(y[i:i + n0])\n",
        "        i += n0\n",
        "    return x_cord, y_cord\n",
        "\n",
        "\n",
        "def test_data2():\n",
        "    m = m2\n",
        "    testing_x, testing_y = get_data(m, 100)\n",
        "    return testing_x, testing_y\n",
        "\n",
        "################################################################################\n",
        "def data_3():\n",
        "    m = m3\n",
        "    # print(m)\n",
        "    x, y = get_data(m, sum(n_k_dev))\n",
        "    i = 0\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for n0 in n_k_dev:\n",
        "        x_cord.append(x[i:i + n0])\n",
        "        y_cord.append(y[i:i + n0])\n",
        "        i += n0\n",
        "    return x_cord, y_cord\n",
        "\n",
        "\n",
        "def test_data3():\n",
        "    m = m3\n",
        "    testing_x, testing_y = get_data(m, 100)\n",
        "    return testing_x, testing_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzMGqjB40Tc7"
      },
      "source": [
        "# Client's Side setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGzkwOvwsNem"
      },
      "source": [
        "# Performs client side SGD locally\n",
        "def SGD_(m, n, iteration, x_cord, y_cord, alpha):  \n",
        "    for p in range(iteration):\n",
        "        for i in range(n):\n",
        "            j = random.randint(0, n - 1)\n",
        "            x_i = x_cord[j]\n",
        "            y_a = y_cord[j]\n",
        "            y_p = np.dot(m, x_i)\n",
        "            dm = 2 * x_i * (y_p - y_a)\n",
        "            m = m - alpha * dm\n",
        "    return m\n",
        "\n",
        "\n",
        "def no_of_pts(id):\n",
        "    return n_k_dev[id]\n",
        "\n",
        "\n",
        "def fetch_coords(id, x_cord, y_cord):\n",
        "    return x_cord[id], y_cord[id]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBWhsM_91BFn"
      },
      "source": [
        "## Different local learnings to replicate training of data with different devices having different model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dJc0epL0_uz"
      },
      "source": [
        "def device_local_learning(alp_, w, id, itern):\n",
        "    # print(\"Device %i running\" % id)\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord = data_1()\n",
        "    x_cord, y_cord = fetch_coords(id, x_cord, y_cord)\n",
        "    w = SGD_(w, n_k, itern, x_cord, y_cord, alp_)\n",
        "    save_weight(w, n_k, id)\n",
        "    return w, n_k\n",
        "\n",
        "\n",
        "def device_local_learning_2(alp_, w, id, itern):\n",
        "    # print(\"Device %i running\" % id)\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord = data_2()\n",
        "    x_cord, y_cord = fetch_coords(id, x_cord, y_cord)\n",
        "    w = SGD_(w, n_k, itern, x_cord, y_cord, alp_)\n",
        "    save_weight(w, n_k, id)\n",
        "    return w, n_k\n",
        "\n",
        "\n",
        "# Conducts local training for SGD and outputs weight vector\n",
        "def device_local_learning_3(alp_, w, id, itern):\n",
        "    # print(\"Device %i running\" % id)\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord = data_3()\n",
        "    x_cord, y_cord = fetch_coords(id, x_cord, y_cord)\n",
        "    w = SGD_(w, n_k, itern, x_cord, y_cord, alp_)\n",
        "    save_weight(w, n_k, id)\n",
        "    return w, n_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwTSkkrO1qWS"
      },
      "source": [
        "# Server Side setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0SeU_UFsR6v"
      },
      "source": [
        "#Vanilla Fedrated average for  SGD\n",
        "def federated_avg(w_, n_k):\n",
        "    n_ = np.sum(n_k)\n",
        "    w_k = 0\n",
        "    for p in range(len(n_k)):\n",
        "        w_k += (n_k[p] * w_[p]) / n_\n",
        "    return w_k\n",
        "\n",
        "\n",
        "def SGD_federated_learning_new(T, K, C, w, n, alp_):\n",
        "    T1 = T\n",
        "    w_ = np.zeros((int(C * K), w_n))\n",
        "    n_k = np.zeros(int(C * K))\n",
        "    prob = 0.8\n",
        "    prob2 = 0.95\n",
        "    # Train for T many rounds(Global Model)\n",
        "    for i in range(T1):\n",
        "        # Random devices for traing the model\n",
        "        training_subset = np.random.choice(all_device, int(C * K), replace=False)\n",
        "        z = 0\n",
        "        for j in training_subset:\n",
        "            # To simullate a random fraction of devices that have differnt data set \n",
        "            ran = random.random()\n",
        "            if ran < prob:\n",
        "                w_[z], n_k[z] = device_local_learning(alp_, w, j, n)\n",
        "            elif ran > prob and ran < prob2:\n",
        "                w_[z], n_k[z] = device_local_learning_2(alp_, w, j, n)\n",
        "            else:\n",
        "                w_[z], n_k[z] = device_local_learning_3(alp_, w, j, n)\n",
        "            z += 1\n",
        "        w_k = federated_avg(w_, n_k)\n",
        "        w = w_k\n",
        "        print(\"Global Iteration \", i, \" done.\")\n",
        "\n",
        "\n",
        "    print(\"Initial Model sent.\")\n",
        "    w_global = w\n",
        "    #Sent the weights for clustering\n",
        "    w_k, device_no = new_update(w_, training_subset, n_k, alp_, n, T)\n",
        "    return w_k, w_global, device_no\n",
        "\n",
        "\n",
        "def new_update(w_, training_subset, n_k, alp_, n, T):\n",
        "    w_mult = []\n",
        "    # Clustering of devices via their weight vectors\n",
        "    clustering = DBSCAN(eps=0.5, min_samples=1).fit(w_)\n",
        "    print(\"Labels for devices\",clustering.labels_)\n",
        "    device_number = []\n",
        "    device_nu=0\n",
        "    #DO for all the labels\n",
        "    for i in range(np.max(clustering.labels_) + 1):\n",
        "        training_subset_n = []\n",
        "        n_k_n = []\n",
        "        w_new = []\n",
        "        #Make a subset of traing subset taht have same cluster-id\n",
        "        for k in range(len(training_subset)):\n",
        "            lab = clustering.labels_[k]\n",
        "            if i == lab:\n",
        "                training_subset_n.append(training_subset[k])\n",
        "                n_k_n.append(n_k[k])\n",
        "                w_new.append(w_[k])\n",
        "                device_nu+=1\n",
        "        device_number.append(len(training_subset_n))\n",
        "        #For T many iteration do the P2P traing\n",
        "        for j in range(T):\n",
        "            initiate = np.random.choice(training_subset_n)\n",
        "            w_t = p2p_learn(initiate, training_subset_n, i)\n",
        "            print(\"Class \", i, \" Iteration \", j, \" done.\")\n",
        "        \"\"\"for i in training_subset_n:\n",
        "            i = str(i)\n",
        "            os.remove(i + \"_wn.dat\")\"\"\"\n",
        "        #Save the models for all calsses\n",
        "        w_mult.append(w_t)\n",
        "        print(device_number,device_nu)\n",
        "    return w_mult, device_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuMk6ughXTQi"
      },
      "source": [
        "## Testing of the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oi9rNZeqsZem"
      },
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "\n",
        "def average_testing_error(w, testing_x, testing_y):\n",
        "    error = 0\n",
        "    for i in range(len(testing_x)):\n",
        "        y_p = np.dot(w, testing_x[i])\n",
        "        y_a = testing_y[i]\n",
        "        # print(i)\n",
        "        error += (y_a - y_p) ** 2\n",
        "    error *= (1 / len(testing_x))\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBlQimWMsaam"
      },
      "source": [
        "#Request Other device for their weights\n",
        "def request_dat(initiate):\n",
        "    fl = str(initiate)\n",
        "    w_n = np.loadtxt(fl + \"_wn.dat\")\n",
        "    n_k_new = n_k_devices[initiate]\n",
        "    return w_n, n_k_new\n",
        "\n",
        "#Initiate P2P learn by making a traing subset which doesnot include the device which is calling\n",
        "def p2p_learn(initate, training_subset, clas):\n",
        "    w_n, n_k_new = request_dat(initate)\n",
        "    training_subset_c = training_subset.copy()\n",
        "    training_subset_c.remove(initate)\n",
        "    w_new = communicate(w_n, training_subset_c, n_k_new, clas)\n",
        "    return w_new\n",
        "\n",
        "# Initiate communication and does the aggreagtion\n",
        "def communicate(w_o, traiinng_subset, n_k, clas):\n",
        "    print(traiinng_subset)\n",
        "    #Take a random subset of traing subset and call them \n",
        "    comm_len = np.random.randint(1, len(traiinng_subset))\n",
        "    call_to = np.random.choice(traiinng_subset, comm_len)\n",
        "    print(\"Call\", call_to)\n",
        "\n",
        "    #Agrregate all the weights available\n",
        "    w_ = []\n",
        "    n_ = []\n",
        "    w_.append(w_o)\n",
        "    n_.append(n_k)\n",
        "    for i in call_to:\n",
        "        w_w, n_w = request_dat(i)\n",
        "        w_.append(w_w)\n",
        "        n_.append(n_w)\n",
        "    w_new = federated_avg(w_, n_)\n",
        "    \n",
        "    #Compares old and new weights to decide which one performs better\n",
        "    if clas == 1:\n",
        "        testing_x1, testing_y1 = test_data1()\n",
        "        err_o = average_testing_error(w_o, testing_x1, testing_y1)\n",
        "        err_1 = average_testing_error(w_new, testing_x1, testing_y1)\n",
        "    elif clas == 2:\n",
        "        testing_x2, testing_y2 = test_data2()\n",
        "        err_o = average_testing_error(w_o, testing_x2, testing_y2)\n",
        "        err_1 = average_testing_error(w_new, testing_x2, testing_y2)\n",
        "    else:\n",
        "        testing_x3, testing_y3 = test_data3()\n",
        "        err_o = average_testing_error(w_o, testing_x3, testing_y3)\n",
        "        err_1 = average_testing_error(w_new, testing_x3, testing_y3)\n",
        "        \n",
        "    if err_1 < err_o:\n",
        "        w_nn = w_new\n",
        "    else:\n",
        "        w_nn = w_o\n",
        "\n",
        "    m=0\n",
        "\n",
        "    #Save new model\n",
        "    for i in call_to:\n",
        "        n_k_=n_[m]\n",
        "        save_weight(w_nn,n_k_,i)\n",
        "        m+=1\n",
        "\n",
        "    return w_nn\n",
        "\n",
        "#For Saving the model/Weights Loacally\n",
        "def save_weight(w, n_k, id):\n",
        "    n_k_devices[id] = n_k\n",
        "    fl = str(id)\n",
        "    file = open(fl + \"_wn.dat\", \"w\")\n",
        "    w_p = [w]\n",
        "    for row in w_p:\n",
        "        np.savetxt(file, row)\n",
        "    file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsPX4yHDsi0I",
        "outputId": "6b745c0b-c5dc-4a75-ea43-2f1a5c080a8a"
      },
      "source": [
        "#Three different weights to represent different data\n",
        "m1 = [1.5, 12, 4.0, 2.9, 4.8, 6.1, 1]\n",
        "m2 = [1, 12, 4, 3, 5, 6, 1.2]\n",
        "m3 = [1.1, 12.2, 3.9, 3, 4.8, 5.8, 0.7]\n",
        "k = 500\n",
        "n_k_dev = np.random.randint(100, 1000, k)\n",
        "w_n = 7  # cardinality of weight vector\n",
        "\n",
        "T = 10  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "C = 0.25  # fraction of nodes used at each iteration\n",
        "# B =K*C #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(w_n)\n",
        "n = 50  # number of iteration for local training before pulling\n",
        "alp_ = 0.01  # local learning rate\n",
        "n_k = random.randint(10, 100)\n",
        "n_k_devices = np.zeros(K)\n",
        "all_device = np.arange(K)\n",
        "# x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w1, w_global, device_no = SGD_federated_learning_new(T, K, C, w, n, alp_)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Iteration  0  done.\n",
            "Initial Model sent.\n",
            "Labels for devices [0 0 1 0 1 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 2 0 1 1 0 0 0 2 0 1 0 0 0 0 0 0\n",
            " 0 1 2 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 2 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[289, 21, 390, 393, 127, 168, 464, 488, 183, 271, 109, 385, 216, 353, 222, 478, 38, 487, 115, 446, 6, 254, 291, 355, 84, 150, 211, 223, 67, 158, 369, 42, 197, 226, 372, 206, 138, 405, 35, 33, 470, 274, 13, 329, 484, 461, 497, 499, 48, 152, 188, 335, 144, 434, 63, 175, 154, 230, 332, 269, 64, 384, 394, 117, 379, 102, 88, 60, 324, 82, 15, 174, 462, 163, 493, 227, 331, 281, 68, 257, 396, 475, 312, 243, 59, 366, 98, 354, 314, 156, 270, 50, 74, 204, 217, 252, 308, 275, 295, 294, 460]\n",
            "Call [ 50  33 168 150 487 117 332 294 217 152 294 493 308 379 484  63 197 497\n",
            " 353 487 329 174  35  84 204 163 332 329 222]\n",
            "Class  0  Iteration  0  done.\n",
            "[102] 102\n",
            "[375, 358, 91, 210, 24, 277, 315, 284, 65, 370, 219, 128, 14, 456]\n",
            "Call [ 24 358  65 277 456  91 315 219 370 370 128  24]\n",
            "Class  1  Iteration  0  done.\n",
            "[102, 15] 117\n",
            "[436, 189, 177, 290, 247, 187, 97]\n",
            "Call [ 97 247 247 436 177 189]\n",
            "Class  2  Iteration  0  done.\n",
            "[102, 15, 8] 125\n",
            "Time taken for SGD federated learning 215.85918712615967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEk2pwPlsmZW",
        "outputId": "8a4e159c-b4ed-4296-acd3-2046a14d6fd7"
      },
      "source": [
        "print(\"Original weigts for minority(I) is \", m1, \" and minority(II) \", m2, \" and majority is:\", m3)\n",
        "print(\"Global model has weights:\", w_global)\n",
        "print(\"Classed multi model has weights:\", w1)\n",
        "print(\"Clustering number\", device_no)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weigts for minority(I) is  [1.5, 12, 4.0, 2.9, 4.8, 6.1, 1]  and minority(II)  [1, 12, 4, 3, 5, 6, 1.2]  and majority is: [1.1, 12.2, 3.9, 3, 4.8, 5.8, 0.7]\n",
            "Global model has weights: [ 1.4054721  12.01300206  3.99349623  2.92021045  4.82740854  6.0667765\n",
            "  1.00789139]\n",
            "Classed multi model has weights: [array([ 1.5       , 12.        ,  3.99999999,  2.9       ,  4.79999999,\n",
            "        6.09999999,  1.00000002]), array([ 1. , 12. ,  4. ,  3. ,  5. ,  6. ,  1.2]), array([ 1.10000491, 12.19999829,  3.89999774,  2.999999  ,  4.80000236,\n",
            "        5.79999439,  0.70000325])]\n",
            "Clustering number [102, 15, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGxsf7oKsrhO",
        "outputId": "6292560f-b424-4e42-d381-f900a04a217a"
      },
      "source": [
        "m_1 = w1[0]\n",
        "m_2 = w1[1]\n",
        "m_3 = w1[2]\n",
        "#Testing for different multi models for different data \n",
        "testing_x1, testing_y1 = test_data1()\n",
        "testing_x2, testing_y2 = test_data2()\n",
        "testing_x3, testing_y3 = test_data3()\n",
        "print(\"Total number of device = \", int(K * C))\n",
        "print(\"Cluster having the majority has %i devices.\" % device_no[0])\n",
        "print(\"Cluster having the minority has %i devices and %i devices.\" % (device_no[1], device_no[2]))\n",
        "print(\"Average Testing Error for Global model:\\n For majority devices error =\",\n",
        "      average_testing_error(w_global, testing_x1, testing_y1), \"\\n For minority cluster with \", device_no[1],\n",
        "      \" devices has error =\", average_testing_error(w_global, testing_x2, testing_y2), \"\\n For minority cluster with \",\n",
        "      device_no[2], \"devices error =\", average_testing_error(w_global, testing_x3, testing_y3))\n",
        "\n",
        "print(\"Average Testing Error for Separate classified model:\\n For majority devices error\",\n",
        "      average_testing_error(m_3, testing_x3, testing_y3), \"\\n For minority cluster with \", device_no[1],\n",
        "      \" devices has error =\", average_testing_error(m_1, testing_x1, testing_y1), \"\\n For minority cluster with \",\n",
        "      device_no[2], \"devices error =\", average_testing_error(m_2, testing_x2, testing_y2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of device =  125\n",
            "Cluster having the majority has 102 devices.\n",
            "Cluster having the minority has 15 devices and 8 devices.\n",
            "Average Testing Error for Global model:\n",
            " For majority devices error = 0.0022940326985583397 \n",
            " For minority cluster with  15  devices has error = 0.018326042559526037 \n",
            " For minority cluster with  8 devices error = 0.15510179756883888\n",
            "Average Testing Error for Separate classified model:\n",
            " For majority devices error 5.680703753720265e-12 \n",
            " For minority cluster with  15  devices has error = 7.050767717493925e-17 \n",
            " For minority cluster with  8 devices error = 2.46557438511628e-22\n"
          ]
        }
      ]
    }
  ]
}