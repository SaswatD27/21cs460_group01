{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Subclustering MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaswatD27/21cs460_group01/blob/main/Code/Subclustering_MNIST(Not%20Working%20Yet).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_cGo_SSDYEJ"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from tensorflow_federated import federated_mean\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow import keras\n",
        "from copy import deepcopy\n",
        "from sklearn.cluster import DBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFpbNcCx9bSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836a1c3d-7fa9-4159-b586-2ca46f15ec51"
      },
      "source": [
        "#(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "#emnist_train, emnist_test = datasets.emnist.load_data()\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "img_rows, img_cols = 28, 28"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIMGSiYE_gIH"
      },
      "source": [
        "K=100\n",
        "n_k= np.random.randint(100,600,K)\n",
        "#n_k=[600 for i in range(K)]\n",
        "n_tot=sum(n_k)\n",
        "client_images=deepcopy(list(range(len(train_images))))\n",
        "random.shuffle(client_images)\n",
        "#print(lenMNIST)\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "'''\n",
        "for i in range(K):\n",
        "  x_train.append([])\n",
        "  y_train.append([])\n",
        "  for j in range(n_k[i]):\n",
        "    k=client_images.pop()\n",
        "    x_train[i].append(train_images[k])\n",
        "    y_train[i].append(train_labels[k])\n",
        "'''\n",
        "indices_dev=[]\n",
        "for i in range(K):\n",
        "  indices_dev.append([])\n",
        "  for j in range(n_k[i]):\n",
        "    k=client_images.pop()\n",
        "    indices_dev[i].append(k)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azLD3KMkohh1",
        "outputId": "a4ed375c-ce61-4c66-d731-e4e061086f3a"
      },
      "source": [
        "print(n_tot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnfYdOaF9nWE"
      },
      "source": [
        "#works!\n",
        "def create_DNN():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  #model.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "  #model.add(tf.keras.layers.BatchNormalization())\n",
        "  #model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Dense(units=10, activation='softmax')) \n",
        "  #model.add(tf.keras.layers.BatchNormalization())\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7Tk3Eq29roM"
      },
      "source": [
        "model=create_DNN()\n",
        "model.compile(optimizer='adam',\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(np.array(model.get_weights()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XwjNE0-DeSl"
      },
      "source": [
        "def device_local_learning(server_model,id): #Conducts local training for SGD and outputs weight vector\n",
        "    model=server_model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=['accuracy'])\n",
        "    history = model.fit(train_images[indices_dev[id]],train_labels[indices_dev[id]], epochs=20, validation_data=(test_images, test_labels), verbose=0)\n",
        "    #print(id,\" done.\")\n",
        "    return model, n_k[id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj-G62xM9M-W"
      },
      "source": [
        "def avg_weights(client_frac,client_models,n_clients_round):\n",
        "  client_models=np.array(client_models)\n",
        "  res=client_frac[0]*np.array(client_models[0].get_weights())\n",
        "  #print(res)\n",
        "  for i in range(1,n_clients_round):\n",
        "    res+=client_frac[i]*np.array(client_models[i].get_weights())\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXDeZWHf_21O"
      },
      "source": [
        "#FED_AVG_DNN : WORKS NOW\n",
        "def fed_avg_DNN(server_model, client_models, client_n):\n",
        "  client_frac=np.zeros(len(client_n))\n",
        "  for i in range(len(client_n)):\n",
        "    client_frac[i]=(client_n[i])*(1/np.sum(client_n))\n",
        "  #print(\"Sum of client_frac = \",np.sum(client_frac),\" client_frac = \",client_frac)\n",
        "  #client_weights_sum=np.sum(client_models)\n",
        "  #for layer in server_model.layers:\n",
        "  #  layer.set_weights([np.dot(client_frac,client_model.get_weights()[0]),np.dot(client_frac,client_model.get_weights()[1])])\n",
        "  #server_model.set_weights([np.dot(client_frac,[m.get_weights()[0] for m in client_models]),np.dot(client_frac,[m.get_weights()[1] for m in client_models])])\n",
        "  server_model.set_weights(avg_weights(client_frac,client_models,len(client_frac)))\n",
        "  return server_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgoHgEL9UY32"
      },
      "source": [
        "##Subclustering Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KedU3l3Fehgr"
      },
      "source": [
        "def federated_learning_new(T, K, C, server_model):\n",
        "    w=server_model\n",
        "    T1 = T\n",
        "    #w_ = np.zeros((int(C * K), w_n))\n",
        "    #n_k = np.zeros(int(C * K))\n",
        "    client_models=[]\n",
        "    client_n=[]\n",
        "    # Train for T many rounds\n",
        "    for i in range(T1):\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        z = 0\n",
        "        for j in training_subset:\n",
        "          client_models.append(device_local_learning(w, j)[0])\n",
        "          client_n.append(device_local_learning(w,j)[1])\n",
        "          z += 1\n",
        "        w_k = fed_avg_DNN(server_model, client_models, client_n)\n",
        "        #w = w_k\n",
        "        print(\"Global Iteration \",i,\" done.\")\n",
        "    #print(w_)\n",
        "    w_global=w_k\n",
        "    w_k = new_update(client_models, training_subset, client_n, T)\n",
        "    return w_k,w_global\n",
        "\n",
        "\n",
        "def new_update(w_, training_subset, n_k, T):\n",
        "    print(w_)\n",
        "    w_mult = []\n",
        "    clustering = DBSCAN(eps=0.5, min_samples=1).fit(np.array([tf.make_ndarray(v).flatten() for v in w_],dtype=object))\n",
        "    print(clustering.labels_)\n",
        "\n",
        "    for i in range(np.max(clustering.labels_) + 1):\n",
        "        training_subset_n = []\n",
        "        n_k_n = []\n",
        "        w_new = []\n",
        "        for k in range(len(training_subset)):\n",
        "            lab = clustering.labels_[k]\n",
        "            if i == lab:\n",
        "                training_subset_n.append(training_subset[k])\n",
        "                n_k_n.append(n_k[k])\n",
        "                w_new.append(w_[k])\n",
        "\n",
        "        wt = fed_avg_DNN(server_model, w_new, n_k_n)\n",
        "        n_k_t = []\n",
        "        w_t = []\n",
        "        for j in range(T):\n",
        "            z = 0\n",
        "            for m_t in training_subset_n:\n",
        "              w_t.append(device_local_learning(wt, m_t)[0])\n",
        "              n_k_t.append(device_local_learning(wt, m_t)[1])\n",
        "            w_k_t = fed_avg_DNN(server_model, w_new, n_k_n)\n",
        "            #wt = w_k_t\n",
        "            print(\"Class \", i,\" Iteration \",j,\" done.\")\n",
        "        w_mult.append(w_k_t)\n",
        "\n",
        "    return w_mult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSyWzQSqepT0"
      },
      "source": [
        "def average_testing_error(w, testing_x, testing_y):\n",
        "    error = 0\n",
        "    for i in range(len(testing_x)):\n",
        "        y_p = np.dot(w, testing_x[i])\n",
        "        y_a = testing_y[i]\n",
        "        # print(i)\n",
        "        error += (y_a - y_p) ** 2\n",
        "    error *= (1 / len(testing_x))\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrFYp4wmlKAC"
      },
      "source": [
        "server_model=create_DNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "yGwP0Y__gYt1",
        "outputId": "71323ef3-de0d-4815-e645-2140a2b9fa28"
      },
      "source": [
        "T = 1  # No of federated learning rounds\n",
        "K = 10  # Total no of nodes\n",
        "C = 0.5  # fraction of nodes used at each iteration\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "# x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w1,w_global = federated_learning_new(T, K, C, server_model)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Iteration  0  done.\n",
            "[<keras.engine.sequential.Sequential object at 0x7fbc404dc6d0>, <keras.engine.sequential.Sequential object at 0x7fbc404dc6d0>, <keras.engine.sequential.Sequential object at 0x7fbc404dc6d0>, <keras.engine.sequential.Sequential object at 0x7fbc404dc6d0>, <keras.engine.sequential.Sequential object at 0x7fbc404dc6d0>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-168b7f178812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtime1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_global\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfederated_learning_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time taken for SGD federated learning\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2802a307d562>\u001b[0m in \u001b[0;36mfederated_learning_new\u001b[0;34m(T, K, C, server_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(w_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mw_global\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mw_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2802a307d562>\u001b[0m in \u001b[0;36mnew_update\u001b[0;34m(w_, training_subset, n_k, T)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mw_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mclustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2802a307d562>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mw_mult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mclustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclustering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mMakeNdarray\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m   \"\"\"\n\u001b[0;32m--> 597\u001b[0;31m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m   \u001b[0mnum_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0mtensor_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'tensor_shape'"
          ]
        }
      ]
    }
  ]
}