{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS460 FL 1.01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaswatD27/21cs460_group01/blob/main/Code/CS460_FL_1_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0HdWCBSThU5"
      },
      "source": [
        "## Notes\n",
        "* Current client side implementation just helps create initial model; no server2client communication yet, no preexisting initial model. Is this necessary? Don't think so, not for now.\n",
        "* Will Noising before Aggregation FL work? Let us see. Perhaps the exponential mechanism will be better. (Implemented, works, hmm... what's the error margin now?)\n",
        "* Need to add error/loss function graph\n",
        "* Need to decouple n_k from training functions to apply some algorithms and make it more natural - Update: Done\n",
        "* IMPORTANT:  Need testing set (might not but still, for propriety; client-side) - Update: Done\n",
        "* Gaussian noise addition (Client side) works but might be identifying, implement mixes/LDP/random subsampling of returned weights?\n",
        "* IMPORTANT: Look at the implementation of Vanilla FedAvg and pass values according to that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdsCx-WWD5Hr"
      },
      "source": [
        "def error_plot(k, train_err, test_err, title1, title2, x_title):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(k, train_err, color='Black')\n",
        "    plt.xlabel(x_title)\n",
        "    plt.ylabel('Average Training Error')\n",
        "    plt.title(title1, y=1.08)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(k, test_err, color='Blue')\n",
        "    plt.xlabel(x_title)\n",
        "    plt.ylabel('Average Testing Error')\n",
        "    plt.title(title2, y=1.08)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E58-dIg2yGo"
      },
      "source": [
        "## Client Side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_cGo_SSDYEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33bdc15-c3b5-4674-fb71-f39b5be01d41"
      },
      "source": [
        "import random\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "w_n=7 #cardinality of weight vector\n",
        "\n",
        "def get_data(m, n): #Generates data with linear trend\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for i in range(n):\n",
        "        x = np.random.rand(w_n)\n",
        "        y = np.dot(m,x)\n",
        "        x_cord.append(x)\n",
        "        y_cord.append(y)\n",
        "    return x_cord, y_cord\n",
        "\n",
        "k=500\n",
        "#m=np.random.rand(w_n) #slope parameter\n",
        "#m[0]=1\n",
        "m=[1,2,4,3,5,6,1.2]\n",
        "print(m)\n",
        "n_k_dev= np.random.randint(100,1000,k)\n",
        "x, y = get_data(m, sum(n_k_dev))\n",
        "i=0\n",
        "x_cord = []\n",
        "y_cord = []\n",
        "\n",
        "testing_x, testing_y = get_data(m, 100)\n",
        "\n",
        "for n0 in n_k_dev:\n",
        "    x_cord.append(x[i:i+n0])\n",
        "    y_cord.append(y[i:i+n0])\n",
        "    i+=n0\n",
        "\n",
        "#print(x_cord, y_cord)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XwjNE0-DeSl"
      },
      "source": [
        "def SGD_(m, n, iteration, x_cord, y_cord, alpha): #Performs client side SGD\n",
        "    for p in range(iteration):\n",
        "        for i in range(n):\n",
        "            #print((p/iteration)*100,(i*100)/n)\n",
        "            j = random.randint(0, n - 1)\n",
        "            x_i = x_cord[j]\n",
        "            y_a = y_cord[j]\n",
        "            y_p = np.dot(m,x_i)\n",
        "            # error = (y_p - y_a) ** 2\n",
        "            dm = 2 * x_i * (y_p - y_a)\n",
        "            m = m - alpha * dm\n",
        "    return m\n",
        "\n",
        "\"\"\"\n",
        "def dat_plot(x_cord, y_cord, m1, c1, m2, c2, n, title1, title2): #Plots scatter/line graphs; displaying results of learning\n",
        "    x_ = []\n",
        "    y_1 = []\n",
        "    y_2 = []\n",
        "    for i in range(n):\n",
        "        x = x_cord[i]\n",
        "        y1 = m1 * x + c1\n",
        "        y2 = m2 * x + c2\n",
        "        x_.append(x)\n",
        "        y_1.append(y1)\n",
        "        y_2.append(y2)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(x_cord, y_cord)\n",
        "    plt.plot(x_, y_1, color='Black')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title(title1)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(x_cord, y_cord)\n",
        "    plt.plot(x_, y_2, color='Blue')\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title(title2)\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "\n",
        "def no_of_pts(id):\n",
        "  return n_k_dev[id]\n",
        "\n",
        "def fetch_coords(id):\n",
        "  return x_cord[id], y_cord[id]\n",
        "\n",
        "def device_local_learning(alp_, w, id, itern): #Conducts local training for SGD and outputs weight vector\n",
        "    #m = 5\n",
        "    #n_k = random.randint(1, 100)\n",
        "    #x_cord, y_cord = get_data(m, n_k)\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w = SGD_(w, n_k, itern, x_cord, y_cord, alp_)\n",
        "    #print(id,w)\n",
        "    # plot(x_cord,y_cord,w_,b_,n_k)\n",
        "    return w, n_k"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDHd2XNC4ZQc"
      },
      "source": [
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "x_cordflat=flatten(x_cord)\n",
        "y_cordflat=flatten(y_cord)\n",
        "\n",
        "def average_training_error(w):\n",
        "  error=0\n",
        "  for i in range(sum(n_k_dev)):\n",
        "    y_p=np.dot(w,x_cordflat[i])\n",
        "    y_a=y_cordflat[i]\n",
        "    #print(i)\n",
        "    error+=(y_a-y_p)**2\n",
        "  error*=(1/sum(n_k_dev))\n",
        "  return error\n",
        "\n",
        "def average_testing_error(w):\n",
        "  error=0\n",
        "  for i in range(len(testing_x)):\n",
        "    y_p=np.dot(w,testing_x[i])\n",
        "    y_a=testing_y[i]\n",
        "    #print(i)\n",
        "    error+=(y_a-y_p)**2\n",
        "  error*=(1/len(testing_x))\n",
        "  return error"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGM51V4lhMhZ",
        "outputId": "678762fc-d6eb-4f9c-8521-bca31f7de5b8"
      },
      "source": [
        "print(len(x_cordflat))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zLsgnwlGGAe"
      },
      "source": [
        "## 0. Centralised Learning (as a baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "hcZaMo-5GMFa",
        "outputId": "88ce5760-e5a0-457f-a861-800cd919b135"
      },
      "source": [
        "def centralised_learning(x_cord,y_cord, w_n, itern, alpha):\n",
        "  x_flat=flatten(x_cord)\n",
        "  y_flat=flatten(y_cord)\n",
        "  w=np.random.rand(w_n)\n",
        "  w_fin=SGD_(w,len(x_flat), itern, x_flat, y_flat, alpha)\n",
        "  return w_fin\n",
        "\n",
        "itern=100\n",
        "alpha=0.01\n",
        "time1=time.time()\n",
        "w0=centralised_learning(x_cord,y_cord,w_n,itern,alpha)\n",
        "time2=time.time()\n",
        "print(w0,\"\\n Time taken = \",time2-time1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-628cae036715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtime1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcentralised_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_cord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtime2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\\n Time taken = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-628cae036715>\u001b[0m in \u001b[0;36mcentralised_learning\u001b[0;34m(x_cord, y_cord, w_n, itern, alpha)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0my_flat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mw_fin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mw_fin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e2afb7734720>\u001b[0m in \u001b[0;36mSGD_\u001b[0;34m(m, n, iteration, x_cord, y_cord, alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# error = (y_p - y_a) ** 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8fVC9fUOf5J",
        "outputId": "03f7fada-04e6-4860-e8d1-63dbf8cf4444"
      },
      "source": [
        "#w0=[1, 0.00178753, 0.50501066, 0.67235608, 0.98434998, 0.45182414, 0.85024702] \n",
        "print(\"Average Training Error for Centralised Learning on SGD = \",average_training_error(w0),\"\\nAverage Testing Error for Centralised Learning on SGD = \", average_testing_error(w0))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for Centralised Learning on SGD =  4.209352931600863e-29 \n",
            "Average Testing Error for Centralised Learning on SGD =  4.0894549326657255e-29\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Wxa0OlT0b8"
      },
      "source": [
        "## Server Side\n",
        "### 1. FedAvg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWiOee8WRI2g"
      },
      "source": [
        "def federated_avg(w_, n_k):\n",
        "    n_ = np.sum(n_k)\n",
        "    w_k = 0\n",
        "    for p in range(len(n_k)):\n",
        "        w_k += (n_k[p] * w_[p]) / n_\n",
        "    return w_k\n",
        "\n",
        "\n",
        "def SGD_federated_learning(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    w_ = np.zeros((int(C * K * T), w_n))\n",
        "    n_k = np.zeros(int(C * K * T))\n",
        "    z=0\n",
        "    for i in range(T):\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_local_learning(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        print(i,\" done.\")\n",
        "    w_k = federated_avg(w_, n_k)\n",
        "    return w_k\n",
        "\n",
        "def SGD_federated_learning_plt(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    w_k=[]\n",
        "    for i in range(T):\n",
        "        w_ = []\n",
        "        n_k = []\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        z=0\n",
        "        for j in training_subset:\n",
        "            dev_res = device_local_learning(alp_, w, j, n)\n",
        "            w_.append(dev_res[0])\n",
        "            n_k.append(dev_res[1])\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        w_k.append(federated_avg(training_subset, w_, n_k))\n",
        "        print(i,\" done.\")\n",
        "    return w_k"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UBFg7nx22rf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cc6383a-1efa-4793-89d1-21dd950b54bb"
      },
      "source": [
        "T = 5  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(7)\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w1 = SGD_federated_learning(T, K, C, w, n, alp_)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w1)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w1, b1, n_k, title1=\"FedAvg on SGD\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  done.\n",
            "1  done.\n",
            "2  done.\n",
            "Time taken for SGD federated learning 201.61977124214172 \n",
            " w = [1.00000002 2.         3.99999999 3.00000001 4.99999999 5.99999999\n",
            " 1.2       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsA1Eb5y5VeT",
        "outputId": "4310276b-c544-4d56-c19b-9fd3eb890c77"
      },
      "source": [
        "print(\"Average Training Error for Vanilla FedAvg on SGD = \",average_training_error(w1),\"\\nAverage Testing Error for Vanilla FedAvg on SGD = \", average_testing_error(w1))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for Vanilla FedAvg on SGD =  6.032714489677551e-17 \n",
            "Average Testing Error for Vanilla FedAvg on SGD =  6.132554212538585e-17\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "S_IIH3vrFx6u",
        "outputId": "4ce7c843-311e-405f-cca2-9bc8855cdc96"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "T=100\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(7)\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "FedAvg_training_errors=[]\n",
        "FedAvg_testing_errors=[]\n",
        "w1 = SGD_federated_learning_plt(T, K, C, w, n, alp_)\n",
        "for w_iter in w1:\n",
        "  FedAvg_training_errors.append(average_training_error(w_iter))\n",
        "  FedAvg_testing_errors.append(average_testing_error(w_iter))\n",
        "#print(T1,\" done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  done.\n",
            "1  done.\n",
            "2  done.\n",
            "3  done.\n",
            "4  done.\n",
            "5  done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-30f3a90f970e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mFedAvg_training_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mFedAvg_testing_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_federated_learning_plt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mFedAvg_training_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_training_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0b05d796bfb1>\u001b[0m in \u001b[0;36mSGD_federated_learning_plt\u001b[0;34m(T, K, C, w, n, alp_)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdev_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_local_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malp_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mw_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mn_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e2afb7734720>\u001b[0m in \u001b[0;36mdevice_local_learning\u001b[0;34m(alp_, w, id, itern)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mn_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_of_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m#print(id,w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# plot(x_cord,y_cord,w_,b_,n_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e2afb7734720>\u001b[0m in \u001b[0;36mSGD_\u001b[0;34m(m, n, iteration, x_cord, y_cord, alpha)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mx_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_cord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0my_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# error = (y_p - y_a) ** 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IvSwYw9IEj9"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "print(w1)\n",
        "print(FedAvg_training_errors)\n",
        "error_plot(np.arange(1,10+1,1), FedAvg_training_errors, FedAvg_testing_errors, \"Training Error for Vanilla FedAvg\", \"Testing Error for Vanilla FedAvg\", \"Number of Rounds (T)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOjm9_6xMsC2"
      },
      "source": [
        "##2. FedAvg with Gaussian Noise Added ($(\\varepsilon,\\delta)-$DP)\n",
        "A.K.A Noising before Aggregation FL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKb-S3VKTRGA"
      },
      "source": [
        "def device_nbafederated_learning(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    delta=0.01\n",
        "    eps=70\n",
        "    C=1.01*max([abs(i) for i in m])\n",
        "    w=np.zeros(len(w_))\n",
        "    for i in range(len(w)):\n",
        "      w[i]=w_[i]*(1/(max([1,max(w_)/C])))\n",
        "    #return w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w)), n_k\n",
        "    return w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2)), n_k"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HylQxRyy_ku"
      },
      "source": [
        "def SGD_nbafederated_learning(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    w_ = np.zeros((int(C * K * T), w_n))\n",
        "    n_k = np.zeros(int(C * K * T))\n",
        "    z=0\n",
        "    for i in range(T):\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_nbafederated_learning(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "    w_k = federated_avg(w_[:z], n_k[:z])\n",
        "    return w_k\n",
        "\n",
        "def SGD_nbafederated_learning_plt(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    w_k=[]\n",
        "    for i in range(T):\n",
        "        w_ = []\n",
        "        n_k = []\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        z=0\n",
        "        for j in training_subset:\n",
        "            dev_res = device_nbafederated_learning(alp_, w, j, n)\n",
        "            w_.append(dev_res[0])\n",
        "            n_k.append(dev_res[1])\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        w_k.append(federated_avg(training_subset, w_, n_k))\n",
        "        print(i,\" done.\")\n",
        "    return w_k"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct6QYs_PzRFL",
        "outputId": "6b4a8a96-b7a5-4bf7-ac8a-0335a42399de"
      },
      "source": [
        "T = 5  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100 # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w2 = SGD_nbafederated_learning(T, K, C, w, n, alp_)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w2)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning 333.9982795715332 \n",
            " w = [0.99995165 1.99995165 3.99995165 2.99995165 4.99995164 5.99995164\n",
            " 1.19995165]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH7iSwIG9CEU",
        "outputId": "752dafa3-b308-4f45-897a-481c5dc13b88"
      },
      "source": [
        "print(\"Average Training Error for NbA FedAvg on SGD = \",average_training_error(w2),\"\\nAverage Testing Error for NbA FedAvg on SGD = \", average_testing_error(w2))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for NbA FedAvg on SGD =  2.998740672656647e-08 \n",
            "Average Testing Error for NbA FedAvg on SGD =  2.9509814243729675e-08\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "1-1pEFpQbmxa",
        "outputId": "dcf3a8d3-dcc5-4f4c-8db7-d6f5a89667cf"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "T=10\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(7)\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "NbAFL_training_errors=[]\n",
        "NbAFL_testing_errors=[]\n",
        "w2 = SGD_nbafederated_learning_plt(T, K, C, w, n, alp_)\n",
        "for w in w2:\n",
        "  NbAFL_training_errors.append(average_training_error(w))\n",
        "  NbAFL_testing_errors.append(average_testing_error(w))\n",
        "#print(T1,\" done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-fab0d2450e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mNbAFL_training_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mNbAFL_testing_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_nbafederated_learning_plt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mNbAFL_training_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_training_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-121-3dafbbca0670>\u001b[0m in \u001b[0;36mSGD_nbafederated_learning_plt\u001b[0;34m(T, K, C, w, n, alp_)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mdev_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_nbafederated_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malp_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mw_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mn_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-128-a92409df8e92>\u001b[0m in \u001b[0;36mdevice_nbafederated_learning\u001b[0;34m(alp_, w, id, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mn_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_of_pts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mw_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-e2afb7734720>\u001b[0m in \u001b[0;36mSGD_\u001b[0;34m(m, n, iteration, x_cord, y_cord, alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# error = (y_p - y_a) ** 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuNmiRXucB0r"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "print(NbAFL_training_errors)\n",
        "error_plot(np.arange(1,10+1,1), NbAFL_training_errors, NbAFL_testing_errors, \"Training Error for NbA FedAvg\", \"Testing Error for NbA FedAvg\", \"Number of Rounds (T)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvF1bTZ4cTLv"
      },
      "source": [
        "## 3. Reduce Communication Overhead (Volume)\n",
        "Probabilistic Quantisation\n",
        "\n",
        "Weight Subsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HhSqX1jfsae"
      },
      "source": [
        "def prob_quant(w):\n",
        "  w_min=min(w)\n",
        "  w_max=max(w)\n",
        "  wq=np.zeros(len(w))\n",
        "  for i in range(len(w)):\n",
        "    wq[i]=np.random.choice([w_min,w_max],[(w_max-w[i])/(w_max-w_min),1-((w_max-w[i])/(w_max-w_min))])\n",
        "  return wq\n",
        "def device_redcommfederated_learning(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    delta=0.01\n",
        "    eps=2\n",
        "    C=5.1\n",
        "    w=np.zeros(len(w_))\n",
        "    for i in range(len(w)):\n",
        "      w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w)), n_k\n",
        "    return prob_quant(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NFq15M6cZvu"
      },
      "source": [
        "## 4. Another Attempt : Adding Random Masking\n",
        "With Static Subsampling as earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3uOeSQvONc5"
      },
      "source": [
        "def device_nbafederated_learning2(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    delta=0.01\n",
        "    eps=2\n",
        "    C=5.1\n",
        "    w=np.zeros(len(w_))\n",
        "    for i in range(len(w)):\n",
        "      w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    #print(\"w_g = \",w_g)\n",
        "    s=0.25\n",
        "    #np.random.seed(seed)\n",
        "    ind=np.random.choice(np.arange(len(w)),size=int((1-s)*len(w)))\n",
        "    mask = np.zeros(len(w))\n",
        "    for i in range(len(w)):\n",
        "      if i in ind:\n",
        "        mask[i]=1\n",
        "    #print(w_g*mask)\n",
        "    return w_g*mask,n_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQq8dG9AgRQq"
      },
      "source": [
        "def federated_avg2(training_subset, w_, n_k):\n",
        "    #n_ = np.sum(n_k)*(0.75)\n",
        "    w_k = []\n",
        "    #n_ind=np.zeros(w_n)\n",
        "    for p in range(len(n_k)):\n",
        "        w_k.append(n_k[p] * w_[p])\n",
        "    w_1=np.transpose(w_k)\n",
        "    #update n_ind with n_k\n",
        "    for i in w_1:\n",
        "      for j in range(len(i)):\n",
        "        if i[j]!=0:\n",
        "          i[j]=1\n",
        "      i*=n_k\n",
        "    #n_ind=[np.count_nonzero(i) for i in w_1]\n",
        "    n_ind=[sum(i) for i in w_1]\n",
        "    #print(\"n_ind=\",n_ind)\n",
        "    w=0\n",
        "    for p in range(len(training_subset)):\n",
        "        w+=(n_k[p]*w_[p])/n_ind\n",
        "    return w\n",
        "def SGD_nbafederated_learning2(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    seeds=[]\n",
        "    w_ = np.zeros((int(C * K * T), w_n))\n",
        "    n_k = np.zeros(int(C * K * T))\n",
        "    z=0\n",
        "    for i in range(T):\n",
        "        #np.random.seed()\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_nbafederated_learning2(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "    w_k = federated_avg2(training_subset, w_, n_k)\n",
        "    return w_k\n",
        "\n",
        "def SGD_nbafederated_learning2_plt(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    w_k=[]\n",
        "    for i in range(T):\n",
        "        #np.random.seed()\n",
        "        w_ = []\n",
        "        n_k = []\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        z=0\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            dev_res = device_nbafederated_learning2(alp_, w, j, n)\n",
        "            w_.append(dev_res[0])\n",
        "            n_k.append(dev_res[1])\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        w_k.append(federated_avg2(w_, n_k))\n",
        "        print(i,\" done.\")\n",
        "    return w_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lXy-e90gZG_",
        "outputId": "956c46a8-69c9-40c1-8945-0c9b324913bf"
      },
      "source": [
        "T = 5  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w2 = SGD_nbafederated_learning2(T, K, C, w, n, alp_)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w2)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning 365.5682189464569 \n",
            " w = [1.34437206 2.34183301 3.56094899 4.03797441 5.39339701 5.25008139\n",
            " 1.25352552]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNLf0i5Yggmh",
        "outputId": "8a28abba-9049-44fc-8297-488c641ba98f"
      },
      "source": [
        "print(\"Average Training Error for NbA FedAvg w/ Random Mask on SGD = \",average_training_error(w2),\"\\nAverage Testing Error for NbA FedAvg w/ Random Mask on SGD = \", average_testing_error(w2))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for NbA FedAvg w/ Random Mask on SGD =  0.13101611851098324 \n",
            "Average Testing Error for NbA FedAvg w/ Random Mask on SGD =  0.13036598162280666\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W572aVc6uY6U",
        "outputId": "ff3e6459-e370-4965-94ac-ba7dc726ce0b"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "T=100\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(7)\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "RM_training_errors=[]\n",
        "RM_testing_errors=[]\n",
        "w2 = SGD_nbafederated_learning2_plt(T, K, C, w, n, alp_)\n",
        "for w_iter in w2:\n",
        "  RM_training_errors.append(average_training_error(w_iter))\n",
        "  RM_testing_errors.append(average_testing_error(w_iter))\n",
        "#print(T1,\" done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0  done.\n",
            "1  done.\n",
            "2  done.\n",
            "3  done.\n",
            "4  done.\n",
            "5  done.\n",
            "6  done.\n",
            "7  done.\n",
            "8  done.\n",
            "9  done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v6vct950rZP"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "print(RM_training_errors)\n",
        "error_plot(np.arange(1,10+1,1), RM_training_errors, RM_testing_errors, \"Training Error;\", \"Testing Error for Random Masked, NbA FedAvg\", \"Number of Rounds (T)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbX-yN-qh5xF"
      },
      "source": [
        "## 6. NbAFL with Probabilistic Quantisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQZiF0VNiErb"
      },
      "source": [
        "def prob_quant(w):\n",
        "  w_min=min(w)\n",
        "  w_max=max(w)\n",
        "  wq=np.zeros(len(w))\n",
        "  for i in range(len(w)):\n",
        "    wq[i]=float(np.random.choice([w_min,w_max],p=[((w_max-w[i])/(w_max-w_min)),(1-((w_max-w[i])/(w_max-w_min)))]))\n",
        "  return wq\n",
        "def device_nbafederated_learning_pq(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    delta=0.01\n",
        "    eps=70\n",
        "    C=1.01*max(m)\n",
        "    w=np.zeros(len(w_))\n",
        "    for i in range(len(w)):\n",
        "      w[i]=w_[i]*(1/(max([1,max(w_)/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    return prob_quant(w_g), n_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5usmdogscYX"
      },
      "source": [
        "def SGD_nbafederated_learning_pq(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    for i in range(T):\n",
        "        w_ = np.zeros((int(C * K * T), w_n))\n",
        "        n_k = np.zeros(int(C * K * T))\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        z=0\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_nbafederated_learning_pq(alp_, w, j, n)\n",
        "            #print(i,z)\n",
        "            z+=1\n",
        "    w_k = federated_avg(training_subset, w_, n_k)\n",
        "    return w_k\n",
        "def SGD_nbafederated_learning_pq_plt(T, K, C, w, n, alp_):\n",
        "    k = np.arange(K)\n",
        "    w_ = []\n",
        "    n_k = []\n",
        "    z=0\n",
        "    # Train for T many rounds\n",
        "    for i in range(T):\n",
        "        training_subset = np.random.randint(0, K, int(C * K))\n",
        "        for j in training_subset:\n",
        "            dev_res = device_nbafederated_learning_pq(alp_, w, j, n)\n",
        "            w.append(dev_res[0])\n",
        "            n_k.append(dev_res[1])\n",
        "            #print(i,z)\n",
        "            z+=1\n",
        "        w_k.append(federated_avg(training_subset, w_[:z], n_k[:z]))\n",
        "        print(i,\" done.\")\n",
        "    return w_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oac1J4HYsnnZ",
        "outputId": "ae7c62ea-6d4a-4303-c451-19a06b3fc731"
      },
      "source": [
        "T = 15  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w_pq = SGD_nbafederated_learning_pq(T, K, C, w, n, alp_)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning with probabilistic binarisation\", time2 - time1,\"\\n w =\",w_pq)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning with probabilistic binarisation 1095.7553567886353 \n",
            " w = [1.00024326 2.14072738 3.97115855 3.20425608 5.23787382 6.00072659\n",
            " 1.21816426]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YNX1wZZwaGl",
        "outputId": "e3c60b72-0a87-4b24-b318-006fb89cf952"
      },
      "source": [
        "print(\"Average Training Error for NbA FedAvg w/ Probabilistic Binarisation on SGD = \",average_training_error(w_pq),\"\\nAverage Testing Error for NbA FedAvg w/ Probabilistic Binarisation on SGD = \", average_testing_error(w_pq))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for NbA FedAvg w/ Probabilistic Binarisation on SGD =  0.09210373561474153 \n",
            "Average Testing Error for NbA FedAvg w/ Probabilistic Binarisation on SGD =  0.09383845621004694\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0_9iuKLs5bQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "0u3sJKqG1ih5",
        "outputId": "1de28316-3b4b-49dc-d271-a9e63ec6af4e"
      },
      "source": [
        "#RUN FOR ERROR PLOT\n",
        "T=100\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = np.random.rand(7)\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "RM_training_errors=[]\n",
        "RM_testing_errors=[]\n",
        "w2 = SGD_nbafederated_learning2_plt(T, K, C, w, n, alp_)\n",
        "for w_iter in w2:\n",
        "  RM_training_errors.append(average_training_error(w_iter))\n",
        "  RM_testing_errors.append(average_testing_error(w_iter))\n",
        "#print(T1,\" done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-a5fb71b1bdd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mRM_training_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mRM_testing_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_nbafederated_learning2_plt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mRM_training_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_training_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-341d9d8ac492>\u001b[0m in \u001b[0;36mSGD_nbafederated_learning2_plt\u001b[0;34m(T, K, C, w, n, alp_)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#np.random.rand(seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_subset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdev_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_nbafederated_learning2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malp_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mw_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mn_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-83621d41a275>\u001b[0m in \u001b[0;36mdevice_nbafederated_learning2\u001b[0;34m(alp_, w, id, n)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;31m#n_k.append(n_k0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfetch_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mw_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-e2afb7734720>\u001b[0m in \u001b[0;36mSGD_\u001b[0;34m(m, n, iteration, x_cord, y_cord, alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m# error = (y_p - y_a) ** 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy4BLeYI0lxC"
      },
      "source": [
        "## 5. Random Masking, with Dynamic Subsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfeG0VE304-s"
      },
      "source": [
        "def device_nbafederated_learning3(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    delta=0.01\n",
        "    eps=2\n",
        "    C=5.1\n",
        "    w=np.zeros(len(w_))\n",
        "    for i in range(len(w)):\n",
        "      w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    #print(\"w_g = \",w_g)\n",
        "    s=0.25\n",
        "    #np.random.seed(seed)\n",
        "    ind=np.random.choice(np.arange(len(w)),size=int((1-s)*len(w)))\n",
        "    mask = np.zeros(len(w))\n",
        "    for i in range(len(w)):\n",
        "      if i in ind:\n",
        "        mask[i]=1\n",
        "    #print(w_g*mask)\n",
        "    return w_g*mask,n_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGaI98lt08aB"
      },
      "source": [
        "def federated_avg3(training_subset, w_, n_k):\n",
        "    #n_ = np.sum(n_k)*(0.75)\n",
        "    w_k = []\n",
        "    #n_ind=np.zeros(w_n)\n",
        "    for p in range(len(n_k)):\n",
        "        w_k.append(n_k[p] * w_[p])\n",
        "    w_1=np.transpose(w_k)\n",
        "    #update n_ind with n_k\n",
        "    for i in w_1:\n",
        "      for j in range(len(i)):\n",
        "        if i[j]!=0:\n",
        "          i[j]=1\n",
        "      i*=n_k\n",
        "    #n_ind=[np.count_nonzero(i) for i in w_1]\n",
        "    n_ind=[sum(i) for i in w_1]\n",
        "    print(\"n_ind=\",n_ind) \n",
        "    w=0\n",
        "    for p in range(len(n_k)):\n",
        "        w+=(n_k[p]*w_[p])/n_ind\n",
        "    return w\n",
        "def SGD_nbafederated_learning3(T, K, C, w, n, alp_, beta):\n",
        "    k = np.arange(K)\n",
        "    w_ = np.zeros((int(c * K * T), w_n))\n",
        "    n_k = np.zeros(int(c * K * T))\n",
        "    z=0\n",
        "    # Train for T many rounds\n",
        "    seeds=[]\n",
        "    for i in range(T):\n",
        "        c=C*(1/math.exp(beta*i))\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(c * K))\n",
        "\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_nbafederated_learning3(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "    w_k = federated_avg3(training_subset, w_[:z], n_k[:z])\n",
        "    return w_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J49Cr56t0_eJ",
        "outputId": "d1bbc336-70ac-4d07-d2a8-a69989bd8726"
      },
      "source": [
        "T = 5  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "beta= 0.05\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w_dyn = SGD_nbafederated_learning3(T, K, C, w, n, alp_, beta)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w_dyn)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_ind= [25810.0, 25540.0, 30010.0, 31974.0, 36586.0, 31664.0, 31155.0]\n",
            "Time taken for SGD federated learning 333.9241933822632 \n",
            " w = [1.13882268 2.08542777 3.96749625 2.73184596 4.96160552 5.05543362\n",
            " 0.61048951]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dHHHrpR1Vub",
        "outputId": "4acfa590-0d54-4093-a686-ef7fb281cbb3"
      },
      "source": [
        "print(\"Average Training Error for NbA FedAvg w/ Random Mask on SGD = \",average_training_error(w_dyn),\"\\nAverage Testing Error for NbA FedAvg w/ Random Mask on SGD = \", average_testing_error(w_dyn))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for NbA FedAvg w/ Random Mask on SGD =  0.7900591789217705 \n",
            "Average Testing Error for NbA FedAvg w/ Random Mask on SGD =  0.9164836574369606\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA_fmddqIGEr"
      },
      "source": [
        "def error_plot(k, sgd_err, mini_err, title1, title2, x_title):\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(k, sgd_err, color='Black')\n",
        "    plt.xlabel(x_title)\n",
        "    plt.ylabel('Average Error')\n",
        "    plt.title(title1)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(k, mini_err, color='Blue')\n",
        "    plt.xlabel(x_title)\n",
        "    plt.ylabel('Average Error')\n",
        "    plt.title(title2)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "PEi8g4NLIN2U",
        "outputId": "05d0ed36-72eb-4189-b2d0-1f3340f041ab"
      },
      "source": [
        "m = 5  # slope parameter\n",
        "T = 1  # No of federated learning rounds\n",
        "C = 0.25 # fraction of nodes used at each iteration\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pulling\n",
        "alp_ = 0.01\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "k_=[]\n",
        "sgd_error=[]\n",
        "mini_error=[]\n",
        "for k_n in range(50,500,50):\n",
        "    n_k_dev = np.random.randint(100, 1000, k_n)\n",
        "    x, y = get_data(m, sum(n_k_dev))\n",
        "    i = 0\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for n0 in n_k_dev:\n",
        "        x_cord.append(x[i:i + n0])\n",
        "        y_cord.append(y[i:i + n0])\n",
        "        i += n0\n",
        "    x_cordflat = flatten(x_cord)\n",
        "    y_cordflat = flatten(y_cord)\n",
        "    k_.append(k_n)\n",
        "    w1, b1 = SGD_federated_learning(T, k_n, C, w, b, n, alp_)\n",
        "    sgd_error.append(average_error(w1, b1))\n",
        "    w2, b2 = Mini_federated_learning(T, k_n, C, w, b, n, alp_)\n",
        "    mini_error.append(average_error(w2,b2))\n",
        "error_plot(k_,sgd_error,mini_error,title1=\"FedAvg on SGD\",title2=\"FedAvg on mini-SGD\",x_title=\"Total number of clients\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-588b1d7e0bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0my_cordflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_cord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mk_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD_federated_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0msgd_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMini_federated_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malp_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SGD_federated_learning() takes 6 positional arguments but 7 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLHUoVwPIelR"
      },
      "source": [
        "c_=[]\n",
        "sgd_error=[]\n",
        "mini_error=[]\n",
        "for c_n in range(20,120,20):\n",
        "    c_n=c_n/100\n",
        "    n_k_dev = np.random.randint(100, 1000, K)\n",
        "    x, y = get_data(m, sum(n_k_dev))\n",
        "    i = 0\n",
        "    x_cord = []\n",
        "    y_cord = []\n",
        "    for n0 in n_k_dev:\n",
        "        x_cord.append(x[i:i + n0])\n",
        "        y_cord.append(y[i:i + n0])\n",
        "        i += n0\n",
        "    x_cordflat = flatten(x_cord)\n",
        "    y_cordflat = flatten(y_cord)\n",
        "    c_.append(c_n)\n",
        "    w1, b1 = SGD_federated_learning(T, K, c_n, w, b, n, alp_)\n",
        "    sgd_error.append(average_error(w1, b1))\n",
        "    w2, b2 = Mini_federated_learning(T, K, c_n, w, b, n, alp_)\n",
        "    mini_error.append(average_error(w2,b2))\n",
        "error_plot(c_,sgd_error,mini_error,title1=\"FedAvg on SGD\",title2=\"FedAvg on mini-SGD\",x_title=\"Fraction of clients used for learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSkoihn85fqU"
      },
      "source": [
        "## 6. Dynamic Subsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2DNRv9B5pI8"
      },
      "source": [
        "def device_federated_learning_dyn_samp(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    #delta=0.01\n",
        "    #eps=2\n",
        "    #C=5.1\n",
        "    #w=np.zeros(len(w_))\n",
        "    #for i in range(len(w)):\n",
        "    #  w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    #print(\"w_g = \",w_g)\n",
        "    #s=0.25\n",
        "    #np.random.seed(seed)\n",
        "    #ind=np.random.choice(np.arange(len(w)),size=int((1-s)*len(w)))\n",
        "    #mask = np.zeros(len(w))\n",
        "    #for i in range(len(w)):\n",
        "    #  if i in ind:\n",
        "    #    mask[i]=1\n",
        "    #print(w_g*mask)\n",
        "    return w_,n_k"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK-i83nB6Cum"
      },
      "source": [
        "def SGD_federated_learning_dyn_samp(T, K, C, w, n, alp_, beta):\n",
        "    k = np.arange(K)\n",
        "    # Train for T many rounds\n",
        "    seeds=[]\n",
        "    w_ = np.zeros((int(C * K * T), w_n))\n",
        "    n_k = np.zeros(int(C * K * T))\n",
        "    z=0\n",
        "    for i in range(T):\n",
        "        c=C*(1/math.exp(beta*i))\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(c * K))\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_federated_learning_dyn_samp(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "    w_k = federated_avg(w_[:z], n_k[:z])\n",
        "    return w_k"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RDwUqIJ6bnL",
        "outputId": "2c16ba66-3876-4c1a-fac6-dd6490c3df09"
      },
      "source": [
        "T = 3  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "beta= 0.05\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w_dyn = SGD_federated_learning_dyn_samp(T, K, C, w, n, alp_, beta)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w_dyn)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning 200.8431317806244 \n",
            " w = [1.00000001 2.00000001 4.         3.00000001 4.99999999 5.99999998\n",
            " 1.2       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt4fbtj172ca",
        "outputId": "c7fe7cac-bf56-44e5-d257-92b1eccee81a"
      },
      "source": [
        "print(\"Average Training Error for FedAvg w/ Dyn Sampling on SGD = \",average_training_error(w_dyn),\"\\nAverage Testing Error for FedAvg w/ Dyn Sampling on SGD = \", average_testing_error(w_dyn))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for FedAvg w/ Dyn Sampling on SGD =  7.850548357312599e-17 \n",
            "Average Testing Error for FedAvg w/ Dyn Sampling on SGD =  8.517489817965552e-17\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ouN1amv8PGX"
      },
      "source": [
        "## 1.* Adaptive Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J7Jdw7S8U89"
      },
      "source": [
        "def device_federated_learning_adap_samp(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    #delta=0.01\n",
        "    #eps=2\n",
        "    #C=5.1\n",
        "    #w=np.zeros(len(w_))\n",
        "    #for i in range(len(w)):\n",
        "    #  w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    #print(\"w_g = \",w_g)\n",
        "    #s=0.25\n",
        "    #np.random.seed(seed)\n",
        "    #ind=np.random.choice(np.arange(len(w)),size=int((1-s)*len(w)))\n",
        "    #mask = np.zeros(len(w))\n",
        "    #for i in range(len(w)):\n",
        "    #  if i in ind:\n",
        "    #    mask[i]=1\n",
        "    #print(w_g*mask)\n",
        "    return w_,n_k"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5_ruC8A8cCe"
      },
      "source": [
        "def SGD_federated_learning_adap_samp(T, K, C, w, n, alp_, beta):\n",
        "    k = np.arange(K)\n",
        "    beta0=beta\n",
        "    error=[]\n",
        "    error_t=0\n",
        "    w_ = np.zeros((int(C * K * T), w_n))\n",
        "    n_k = np.zeros(int(C * K * T))\n",
        "    z=0\n",
        "    # Train for T many rounds\n",
        "    for i in range(T):\n",
        "        c=C*(1/math.exp(beta0*i))\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(c * K))\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_federated_learning_adap_samp(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        w_k = federated_avg(w_[:z], n_k[:z])\n",
        "        error.append(average_testing_error(w_k))\n",
        "        if i>0:\n",
        "          if error[i]>error[i-1]:\n",
        "            beta0*=((error[i]/error[i-1])*(i/i+1))\n",
        "            error_t=error[i-1]\n",
        "        #if beta0!=beta:\n",
        "        #  if error[i]<=error_t:\n",
        "        #    beta0=beta\n",
        "    #w_k = federated_avg(training_subset, w_, n_k)\n",
        "    return w_k"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FXdhAl1-La3",
        "outputId": "8ed3f4b8-d733-4071-ce5c-9764452a2228"
      },
      "source": [
        "T = 3  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "beta= 0.05\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w_dyn = SGD_federated_learning_adap_samp(T, K, C, w, n, alp_, beta)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w_dyn)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning 197.63486433029175 \n",
            " w = [1.         2.         4.         3.         5.         6.\n",
            " 1.20000001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_L33Vj_1h1",
        "outputId": "87adba0a-0672-474e-864b-6a80cdec2808"
      },
      "source": [
        "print(\"Average Training Error for FedAvg w/ Adaptive Sampling on SGD = \",average_training_error(w_dyn),\"\\nAverage Testing Error for FedAvg w/ Adaptive Sampling on SGD = \", average_testing_error(w_dyn))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for FedAvg w/ Adaptive Sampling on SGD =  1.0549056184317912e-17 \n",
            "Average Testing Error for FedAvg w/ Adaptive Sampling on SGD =  1.0036234921747094e-17\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMjCLbgTHpCQ"
      },
      "source": [
        "## 1.1* Adaptive Sampling with NbA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nas55_b8Hmzz"
      },
      "source": [
        "def device_nbafederated_learning_adap_samp(alp_, w, id, n): #Conducts local training for SGD and outputs weight vector\n",
        "    n_k = no_of_pts(id)\n",
        "    #for i in range(T):\n",
        "      #n_k.append(n_k0)\n",
        "    x_cord, y_cord=fetch_coords(id)\n",
        "    w_ = SGD_(w, n_k, n, x_cord, y_cord, alp_)\n",
        "    #delta=0.01\n",
        "    #eps=2\n",
        "    #C=5.1\n",
        "    #w=np.zeros(len(w_))\n",
        "    #for i in range(len(w)):\n",
        "    #  w[i]=w_[i]*(1/(max([1,w_[i]/C])))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25*delta)*(2*C*max(n_k_dev))/100),len(w))\n",
        "    #w_g=w + np.random.normal(0,2*math.log((1.25/delta)*(C**2))/(eps**2),len(w))\n",
        "    #print(\"w_g = \",w_g)\n",
        "    #s=0.25\n",
        "    #np.random.seed(seed)\n",
        "    #ind=np.random.choice(np.arange(len(w)),size=int((1-s)*len(w)))\n",
        "    #mask = np.zeros(len(w))\n",
        "    #for i in range(len(w)):\n",
        "    #  if i in ind:\n",
        "    #    mask[i]=1\n",
        "    #print(w_g*mask)\n",
        "    return w_,n_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRFvZ9oVHm0E"
      },
      "source": [
        "def SGD_federated_learning_ada_samp(T, K, C, w, n, alp_, beta):\n",
        "    k = np.arange(K)\n",
        "    beta0=beta\n",
        "    error=[]\n",
        "    error_t=0\n",
        "    # Train for T many rounds\n",
        "    for i in range(T):\n",
        "        c=C*(1/math.exp(beta0*i))\n",
        "        w_ = np.zeros((int(c * K), w_n))\n",
        "        n_k = np.zeros(int(c * K))\n",
        "        #seed=np.random.randint(1)\n",
        "        #seeds.append(seed)\n",
        "        training_subset = np.random.randint(0, K, int(c * K))\n",
        "        z=0\n",
        "        #np.random.rand(seed)\n",
        "        for j in training_subset:\n",
        "            w_[z], n_k[z] = device_nbafederated_learning_adap_samp(alp_, w, j, n)\n",
        "            #print(z)\n",
        "            z+=1\n",
        "        w_k = federated_avg(training_subset, w_, n_k)\n",
        "        error.append(average_testing_error(w_k))\n",
        "        #if i>0:\n",
        "        #  if error[i]>error[i-1]:\n",
        "        #    beta0*=(error[i-1]/error[i])*2\n",
        "        #    error_t=error[i-1]\n",
        "        #if beta0!=beta:\n",
        "        #  if error[i]<=error_t:\n",
        "        #    beta0=beta\n",
        "    #w_k = federated_avg(training_subset, w_, n_k)\n",
        "    return w_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FCPa2phHm0F",
        "outputId": "b76d8138-3a6f-498a-8475-1862ba70a295"
      },
      "source": [
        "T = 5  # No of federated learning rounds\n",
        "K = 500  # Total no of nodes\n",
        "#C = 0.7  # fraction of nodes used at each iteration\n",
        "C=0.25\n",
        "beta= 0.05\n",
        "# B = #Local batch size used at each learning iteration\n",
        "# random Model\n",
        "w = random.random()\n",
        "b = random.random()\n",
        "n = 100  # number of iteration for local training before pooling\n",
        "alp_ = 0.01  # local learning rate\n",
        "#m = 5\n",
        "n_k = random.randint(10, 100)\n",
        "x_cord1, y_cord1 = get_data(m, n_k)\n",
        "\n",
        "time1 = time.time()\n",
        "w_dyn = SGD_federated_learning_dyn_samp(T, K, C, w, n, alp_, beta)\n",
        "time2 = time.time()\n",
        "print(\"Time taken for SGD federated learning\", time2 - time1,\"\\n w =\",w_dyn)\n",
        "# Device.plot(x_cord, y_cord, w1, b1, n_k)\n",
        "\n",
        "#time1 = time.time()\n",
        "#w2, b2 = Mini_federated_learning(T, K, C, w, b, n, alp_)\n",
        "#time2 = time.time()\n",
        "#print(\"Time taken for Mini-batch SGD federated learning\", time2 - time1)\n",
        "# Device.plot(x_cord, y_cord, w2, b2, n_k)\n",
        "# plt.show()\n",
        "#dat_plot1(x_cord1, y_cord1, w2, b2, n_k, title1=\"NbAFedAvg on SGD\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for SGD federated learning 333.95433926582336 \n",
            " w = [1.0000001  1.99999997 4.00000007 3.00000008 4.99999994 5.99999987\n",
            " 1.19999998]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVIdtEcGHm0G",
        "outputId": "1bed3bcc-2ad8-41d8-9ecb-caef9f6b6646"
      },
      "source": [
        "print(\"Average Training Error for FedAvg w/ Dyn Sampling on SGD = \",average_training_error(w_dyn),\"\\nAverage Testing Error for FedAvg w/ Dyn Sampling on SGD = \", average_testing_error(w_dyn))\n",
        "print(\" Actual weights, m = \",m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Error for FedAvg w/ Dyn Sampling on SGD =  3.7190119635908776e-15 \n",
            "Average Testing Error for FedAvg w/ Dyn Sampling on SGD =  3.060815956838882e-15\n",
            " Actual weights, m =  [1, 2, 4, 3, 5, 6, 1.2]\n"
          ]
        }
      ]
    }
  ]
}